{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "sys.path.append('../..')\n",
    "from helpers.metrics import brier_multi, CalibrationErrors, classification_error\n",
    "from helpers.settings import models_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def load_mnist():\n",
    "    num_classes = 10\n",
    "    (x_train_val, y_train_val), (x_test, y_test) = mnist.load_data()\n",
    "    x_train_val = x_train_val.reshape(x_train_val.shape[0], 784)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 784)\n",
    "\n",
    "    x_train_val = x_train_val.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "\n",
    "    x_train_val /= 255\n",
    "    x_test /= 255\n",
    "\n",
    "    y_train_val = to_categorical(y_train_val, num_classes)\n",
    "    y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "    x_train = x_train_val[:50000]\n",
    "    y_train = y_train_val[:50000]\n",
    "    x_val = x_train_val[50000:]\n",
    "    y_val = y_train_val[50000:]\n",
    "    return (x_train, y_train), (x_val, y_val), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_val, y_val), (x_test, y_test) = load_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization\n",
    "from keras.layers.core import Lambda\n",
    "\n",
    "from keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(num_classes=10):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(200, activation='relu', input_shape=(784,)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(200, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(200, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_models = 25\n",
    "learning_rate = 2.5e-3\n",
    "epochs = 15\n",
    "output_folder = os.path.join(models_folder, 'mnist_mlp')\n",
    "if not os.path.isdir(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "#\n",
    "output_baseline_folder = os.path.join(output_folder, 'baselines')\n",
    "if not os.path.isdir(output_baseline_folder):\n",
    "    os.makedirs(output_baseline_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped training...\n",
      "skipped training...\n",
      "skipped training...\n",
      "skipped training...\n",
      "skipped training...\n",
      "skipped training...\n",
      "skipped training...\n",
      "skipped training...\n",
      "skipped training...\n",
      "skipped training...\n",
      "skipped training...\n",
      "skipped training...\n",
      "skipped training...\n",
      "skipped training...\n",
      "skipped training...\n",
      "skipped training...\n",
      "skipped training...\n",
      "skipped training...\n",
      "skipped training...\n",
      "skipped training...\n",
      "skipped training...\n",
      "skipped training...\n",
      "skipped training...\n",
      "skipped training...\n",
      "skipped training...\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_models):\n",
    "    seed = i+17\n",
    "    # change the random state\n",
    "    np.random.seed(seed)\n",
    "    # creating output folder for each of the baseline models\n",
    "    output_model_folder = os.path.join(output_baseline_folder, str(seed))\n",
    "    if not os.path.isdir(output_model_folder):\n",
    "        os.mkdir(output_model_folder)\n",
    "    else:\n",
    "        print('skipped training...')\n",
    "        continue\n",
    "    # callbacks for training\n",
    "    callbacks = list()\n",
    "    log = CSVLogger(os.path.join(output_model_folder, 'log.csv'))\n",
    "    callbacks.append(log)\n",
    "    checkpoint_path = os.path.join(output_model_folder, 'weights.{epoch:02d}.hdf5')\n",
    "    checkpoint = ModelCheckpoint(checkpoint_path, period=1, save_best_only=False, verbose=True)\n",
    "    callbacks.append(checkpoint)\n",
    "    # initialize and compile the model\n",
    "    model = mlp()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=learning_rate), metrics=['accuracy'])\n",
    "    # train\n",
    "    model.fit(x_train, y_train, batch_size=128, epochs=epochs, verbose=1, callbacks=callbacks, validation_data=(x_val, y_val), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(17, 17+n_models):\n",
    "    log = pd.read_csv(os.path.join(output_baseline_folder, str(i), 'log.csv'))\n",
    "    plt.figure(figsize=(8,2),dpi=100)\n",
    "    plt.subplots_adjust(wspace=0.5)\n",
    "    ax1 = plt.subplot(121)\n",
    "    ax2 = plt.subplot(122)\n",
    "    for metric_type in ['', 'val_']:\n",
    "        for key in log.keys():\n",
    "            if 'loss' in key and 'val' not in key:\n",
    "                p = ax1.plot(log[metric_type+ key], label=metric_type + key+str(i), linewidth=1)\n",
    "                c = p[-1].get_color()\n",
    "                ax1.plot(np.argmin(log[metric_type+key]), np.amin(log[metric_type+key]), \n",
    "                         '*', markersize=10, alpha=0.5, color = c)\n",
    "                ax1.set_title('min val NLL: {0:.3f}'.format(np.amin(log['val_' + key])))\n",
    "                ax1.set_ylabel('NLL')\n",
    "                ax1.set_xlabel('epochs')\n",
    "            if 'acc' in key and 'val' not in key:\n",
    "                p = ax2.plot(log[metric_type + key], label=metric_type + key+str(i), linewidth=1)\n",
    "                c = p[-1].get_color()\n",
    "                ax2.plot(np.argmax(log[metric_type + key]), np.amax(log[metric_type + key]), \n",
    "                         '*', markersize=10, alpha=0.5, color=c)\n",
    "                ax2.set_title('max val acc: {0:.2f}'.format(np.amax(log['val_' + key]*100)))\n",
    "                ax2.set_ylabel('accuracy')\n",
    "                ax2.set_xlabel('epochs')\n",
    "    ax1.grid()\n",
    "    ax2.grid()\n",
    "    ax1.legend()\n",
    "    ax2.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, model_id in enumerate(range(17, 17+n_models)):\n",
    "    model_folder = os.path.join(output_baseline_folder, str(model_id))\n",
    "    test_pred_path = os.path.join(model_folder, 'test_pred.npy')\n",
    "    # if True:\n",
    "    if not os.path.isfile(test_pred_path):\n",
    "        checkpoint_path = glob.glob( model_folder + '/weights.' + str(epochs).zfill(2) + '*.hdf5')[0]\n",
    "        model = mlp()\n",
    "        print('loading weights for model {}...'.format(model_id))\n",
    "        model.load_weights(checkpoint_path)\n",
    "        print('running inference...')\n",
    "        test_pred = model.predict(x_test)\n",
    "        np.save(test_pred_path, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_model_id = 17\n",
    "y_pred_sample = np.load(os.path.join(output_baseline_folder, str(sample_model_id), 'test_pred.npy'))\n",
    "shape = np.shape(y_pred_sample)\n",
    "all_test_preds = np.zeros((n_models, *shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = []\n",
    "for index, model_id in enumerate(range(17, 17+n_models)):\n",
    "    y_pred = np.load(os.path.join(output_baseline_folder, str(model_id), 'test_pred.npy'))\n",
    "    all_test_preds[index] = y_pred\n",
    "    nll = log_loss(y_test, y_pred)\n",
    "    error = classification_error(y_test, y_pred)\n",
    "    br = brier_multi(y_test, y_pred)\n",
    "    calib_erros = CalibrationErrors(y_test, y_pred, bin_size=1 / 20., min_samples=0)\n",
    "    _, _, ece, mce, _ = calib_erros.calculate_calibration_errors()\n",
    "    d.append(OrderedDict({\"model\":model_id, \"nll\": nll, \"brier\": br, \"ece\": ece, \"mce\":mce, \"classification error\": error}))\n",
    "df_baselines = pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    model       nll     brier       ece        mce  classification error\n",
      "0      17  0.085811  0.036027  1.212307  66.113958                  2.29\n",
      "1      18  0.088970  0.035416  1.293205  30.072403                  2.19\n",
      "2      19  0.083936  0.035472  1.317934  26.070374                  2.23\n",
      "3      20  0.090023  0.035112  1.139185  37.549862                  2.10\n",
      "4      21  0.092218  0.035476  1.343740  29.371622                  2.26\n",
      "5      22  0.118141  0.042232  1.489946  20.196827                  2.49\n",
      "6      23  0.097130  0.037205  1.411669  39.474791                  2.29\n",
      "7      24  0.085837  0.035658  1.225365  37.476841                  2.20\n",
      "8      25  0.101171  0.038938  1.412432  38.668972                  2.40\n",
      "9      26  0.105199  0.041657  1.461620  19.777432                  2.57\n",
      "10     27  0.103741  0.040022  1.449721  29.872349                  2.42\n",
      "11     28  0.075225  0.030420  1.017944  62.407127                  1.92\n",
      "12     29  0.078692  0.029996  1.035537  32.658288                  1.89\n",
      "13     30  0.092800  0.035851  1.259849  57.814842                  2.21\n",
      "14     31  0.140643  0.046708  1.713743  39.164832                  2.73\n",
      "15     32  0.111696  0.042825  1.552573  32.555023                  2.70\n",
      "16     33  0.084994  0.033658  1.320660  42.629236                  2.09\n",
      "17     34  0.106997  0.038911  1.432756  37.999850                  2.41\n",
      "18     35  0.092093  0.035220  1.396811  60.147494                  2.22\n",
      "19     36  0.100618  0.036551  1.348854  24.788817                  2.26\n",
      "20     37  0.085100  0.034180  1.181056  70.227861                  2.14\n",
      "21     38  0.094349  0.035589  1.241023  29.721172                  2.08\n",
      "22     39  0.078986  0.030598  1.043884  43.188384                  1.87\n",
      "23     40  0.096331  0.036958  1.397005  68.538457                  2.22\n",
      "24     41  0.078084  0.035312  0.848223  32.993051                  2.19\n"
     ]
    }
   ],
   "source": [
    "print(df_baselines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASELINE RESULTS (TEST SET)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "NLL\n",
      "0.095 ± 0.01\n",
      "--------------------\n",
      "BRIER\n",
      "0.037 ± 0.00\n",
      "--------------------\n",
      "ECE\n",
      "1.302 ± 0.19\n",
      "--------------------\n",
      "MCE\n",
      "40.379 ± 14.80\n",
      "--------------------\n",
      "CLASSIFICATION ERROR\n",
      "2.255 ± 0.22\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "print('Baseline Results (test set)'.upper())\n",
    "print('-'*100)\n",
    "metrics = ['nll', 'brier', 'ece', 'mce', 'classification error']\n",
    "for metric in metrics:\n",
    "    print('{0}'.format(metric.upper()))\n",
    "    print('{0:.3f} \\u00B1 {1:.2f}'.format(np.mean(df_baselines[metric]), np.std(df_baselines[metric])))\n",
    "    print('-'*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bootstrap = 100\n",
    "m_deep_ensemble = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = []\n",
    "for i in range(n_bootstrap):\n",
    "    model_indices = np.random.choice(np.shape(all_test_preds)[0], size=m_deep_ensemble, replace=False)\n",
    "    y_pred = np.mean(np.stack(all_test_preds[model_indices], axis=0), axis=0)\n",
    "    nll = log_loss(y_test, y_pred)\n",
    "    error = classification_error(y_test, y_pred)\n",
    "    br = brier_multi(y_test, y_pred)\n",
    "    calib_erros = CalibrationErrors(y_test, y_pred, bin_size=1 / 20., min_samples=0)\n",
    "    _, _, ece, mce, _ = calib_erros.calculate_calibration_errors()\n",
    "    d.append(OrderedDict({\"model\":model_id, \"nll\": nll, \"brier\": br, \"ece\": ece, \"mce\":mce, \"classification error\": error}))\n",
    "df_deep_ensembles = pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>nll</th>\n",
       "      <th>brier</th>\n",
       "      <th>ece</th>\n",
       "      <th>mce</th>\n",
       "      <th>classification error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>0.045298</td>\n",
       "      <td>0.020605</td>\n",
       "      <td>0.994108</td>\n",
       "      <td>17.561141</td>\n",
       "      <td>1.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>0.043021</td>\n",
       "      <td>0.019940</td>\n",
       "      <td>0.910200</td>\n",
       "      <td>46.833028</td>\n",
       "      <td>1.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>0.041633</td>\n",
       "      <td>0.019777</td>\n",
       "      <td>0.737164</td>\n",
       "      <td>73.195208</td>\n",
       "      <td>1.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>41</td>\n",
       "      <td>0.043183</td>\n",
       "      <td>0.020233</td>\n",
       "      <td>0.772780</td>\n",
       "      <td>27.802001</td>\n",
       "      <td>1.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>41</td>\n",
       "      <td>0.043473</td>\n",
       "      <td>0.020234</td>\n",
       "      <td>0.896324</td>\n",
       "      <td>46.618672</td>\n",
       "      <td>1.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>41</td>\n",
       "      <td>0.043710</td>\n",
       "      <td>0.020170</td>\n",
       "      <td>0.873415</td>\n",
       "      <td>18.763776</td>\n",
       "      <td>1.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>41</td>\n",
       "      <td>0.043257</td>\n",
       "      <td>0.019850</td>\n",
       "      <td>0.845719</td>\n",
       "      <td>71.404660</td>\n",
       "      <td>1.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>41</td>\n",
       "      <td>0.042759</td>\n",
       "      <td>0.020121</td>\n",
       "      <td>0.783511</td>\n",
       "      <td>72.969473</td>\n",
       "      <td>1.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>41</td>\n",
       "      <td>0.042617</td>\n",
       "      <td>0.019757</td>\n",
       "      <td>0.714397</td>\n",
       "      <td>27.754570</td>\n",
       "      <td>1.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>41</td>\n",
       "      <td>0.043812</td>\n",
       "      <td>0.020059</td>\n",
       "      <td>0.946234</td>\n",
       "      <td>26.472617</td>\n",
       "      <td>1.20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    model       nll     brier       ece        mce  classification error\n",
       "0      41  0.045298  0.020605  0.994108  17.561141                  1.27\n",
       "1      41  0.043021  0.019940  0.910200  46.833028                  1.23\n",
       "2      41  0.041633  0.019777  0.737164  73.195208                  1.24\n",
       "3      41  0.043183  0.020233  0.772780  27.802001                  1.33\n",
       "4      41  0.043473  0.020234  0.896324  46.618672                  1.35\n",
       "..    ...       ...       ...       ...        ...                   ...\n",
       "95     41  0.043710  0.020170  0.873415  18.763776                  1.23\n",
       "96     41  0.043257  0.019850  0.845719  71.404660                  1.26\n",
       "97     41  0.042759  0.020121  0.783511  72.969473                  1.33\n",
       "98     41  0.042617  0.019757  0.714397  27.754570                  1.26\n",
       "99     41  0.043812  0.020059  0.946234  26.472617                  1.20\n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_deep_ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEEP ENSEMBLE RESULTS (TEST SET)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "NLL\n",
      "0.044 ± 0.00\n",
      "--------------------\n",
      "BRIER\n",
      "0.020 ± 0.00\n",
      "--------------------\n",
      "ECE\n",
      "0.848 ± 0.08\n",
      "--------------------\n",
      "MCE\n",
      "35.646 ± 17.16\n",
      "--------------------\n",
      "CLASSIFICATION ERROR\n",
      "1.294 ± 0.05\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "print('Deep Ensemble Results (test set)'.upper())\n",
    "print('-'*100)\n",
    "metrics = ['nll', 'brier', 'ece', 'mce', 'classification error']\n",
    "for metric in metrics:\n",
    "    print('{0}'.format(metric.upper()))\n",
    "    print('{0:.3f} \\u00B1 {1:.2f}'.format(np.mean(df_deep_ensembles[metric]), np.std(df_deep_ensembles[metric])))\n",
    "    print('-'*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temperature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "def softmax_t(y_logit, t):\n",
    "    return np.exp(y_logit/t)/np.sum(np.exp(y_logit/t), axis=-1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "def ll_t(t):\n",
    "    y_val_pred = model.predict(x_val)\n",
    "    y_pred_base_logit = np.log(y_val_pred)\n",
    "    y_pred_temp = softmax_t(y_pred_base_logit, t)\n",
    "    ll = log_loss(y_val, y_pred_temp)\n",
    "    return ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.83782233]\n",
      " [1.80262627]\n",
      " [1.83544376]\n",
      " [1.74132693]\n",
      " [1.84628994]\n",
      " [1.88229666]\n",
      " [1.8191161 ]\n",
      " [1.77397803]\n",
      " [1.78677419]\n",
      " [1.79504745]\n",
      " [1.85553103]\n",
      " [1.76736516]\n",
      " [1.861452  ]\n",
      " [1.7241243 ]\n",
      " [1.98756775]\n",
      " [1.95261556]\n",
      " [1.8361855 ]\n",
      " [1.88223574]\n",
      " [1.88062071]\n",
      " [1.87970128]\n",
      " [1.77137869]\n",
      " [1.87127553]\n",
      " [1.83803354]\n",
      " [1.90752737]\n",
      " [1.83653927]]\n"
     ]
    }
   ],
   "source": [
    "optimal_temps = []\n",
    "optimal_temps_path = os.path.join(output_baseline_folder, 'optimal_temps.csv')\n",
    "# if True:\n",
    "if not os.path.isfile(optimal_temps_path):\n",
    "    for index, model_id in enumerate(range(17, 17+ n_models)):\n",
    "        print('finding optimal sigma for model {}...'.format(model_id))\n",
    "        model_folder = os.path.join(output_baseline_folder, str(model_id))\n",
    "        checkpoint_path = glob.glob( model_folder + '/weights.' + str(epochs).zfill(2) + '*.hdf5')[0]\n",
    "        model = mlp()\n",
    "        print('loading weights...')\n",
    "        model.load_weights(checkpoint_path)\n",
    "        print('running optimization...')\n",
    "        xopt = minimize(ll_t, 1, method='bfgs', options={'disp': 1})\n",
    "        optimal_temp = xopt['x'][0]\n",
    "        print('-'*100)\n",
    "        print(model_id, optimal_temp)\n",
    "        optimal_temps.append(optimal_temp)\n",
    "    pd.DataFrame(optimal_temps, columns=['optimal temp']).to_csv(optimal_temps_path, index=False)\n",
    "else:\n",
    "    optimal_temps = pd.read_csv(optimal_temps_path).values\n",
    "print(optimal_temps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, model_id in enumerate(range(17, 17 + n_models)):\n",
    "    model_folder = os.path.join(output_baseline_folder, str(model_id))\n",
    "    output_ts_folder = os.path.join(model_folder, 'temp_scaled')\n",
    "    '''\n",
    "    if not os.path.isdir(output_ts_folder):\n",
    "        os.mkdir(output_ts_folder)\n",
    "    else:\n",
    "        continue\n",
    "    '''\n",
    "    y_pred = np.load(os.path.join(model_folder, 'test_pred.npy'))\n",
    "    optimal_temp = optimal_temps[index]\n",
    "    #\n",
    "    y_pred_logit = np.log(y_pred)\n",
    "    y_pred_ts = softmax_t(y_pred_logit, t=optimal_temp)\n",
    "    #\n",
    "    np.save(os.path.join(output_ts_folder, 'test_pred_ts.npy'), y_pred_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = list()\n",
    "for index, model_id in enumerate(range(17, 17 + n_models)):\n",
    "    model_ppe_folder= os.path.join(output_baseline_folder, str(model_id),  'temp_scaled')\n",
    "    y_pred = np.load(os.path.join(model_ppe_folder, 'test_pred_ts.npy'))\n",
    "    nll = log_loss(y_test, y_pred)\n",
    "    error = classification_error(y_test, y_pred)\n",
    "    br = brier_multi(y_test, y_pred)\n",
    "    calib_erros = CalibrationErrors(y_test, y_pred, bin_size=1 / 20., min_samples=0)\n",
    "    _, _, ece, mce, _ = calib_erros.calculate_calibration_errors()\n",
    "    d.append(OrderedDict({\"model\":model_id, \"nll\": nll, \"brier\": br, \"ece\": ece, \"mce\":mce, \"classification error\": error}))\n",
    "df_ts= pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>nll</th>\n",
       "      <th>brier</th>\n",
       "      <th>ece</th>\n",
       "      <th>mce</th>\n",
       "      <th>classification error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0.071337</td>\n",
       "      <td>0.034025</td>\n",
       "      <td>0.623382</td>\n",
       "      <td>76.294943</td>\n",
       "      <td>2.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0.069826</td>\n",
       "      <td>0.033097</td>\n",
       "      <td>0.364700</td>\n",
       "      <td>73.533693</td>\n",
       "      <td>2.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>0.067577</td>\n",
       "      <td>0.033133</td>\n",
       "      <td>0.501464</td>\n",
       "      <td>37.740637</td>\n",
       "      <td>2.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>0.073305</td>\n",
       "      <td>0.033707</td>\n",
       "      <td>0.347560</td>\n",
       "      <td>70.554627</td>\n",
       "      <td>2.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>0.071443</td>\n",
       "      <td>0.033369</td>\n",
       "      <td>0.477062</td>\n",
       "      <td>26.227307</td>\n",
       "      <td>2.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>0.087290</td>\n",
       "      <td>0.039987</td>\n",
       "      <td>0.486031</td>\n",
       "      <td>72.844945</td>\n",
       "      <td>2.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>0.073361</td>\n",
       "      <td>0.034511</td>\n",
       "      <td>0.287854</td>\n",
       "      <td>11.007471</td>\n",
       "      <td>2.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>24</td>\n",
       "      <td>0.069724</td>\n",
       "      <td>0.033737</td>\n",
       "      <td>0.419690</td>\n",
       "      <td>30.359291</td>\n",
       "      <td>2.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>25</td>\n",
       "      <td>0.078615</td>\n",
       "      <td>0.036777</td>\n",
       "      <td>0.172535</td>\n",
       "      <td>22.989505</td>\n",
       "      <td>2.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>26</td>\n",
       "      <td>0.081919</td>\n",
       "      <td>0.039556</td>\n",
       "      <td>0.418288</td>\n",
       "      <td>28.227966</td>\n",
       "      <td>2.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>27</td>\n",
       "      <td>0.079189</td>\n",
       "      <td>0.037731</td>\n",
       "      <td>0.270231</td>\n",
       "      <td>27.209497</td>\n",
       "      <td>2.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>28</td>\n",
       "      <td>0.062206</td>\n",
       "      <td>0.029186</td>\n",
       "      <td>0.513560</td>\n",
       "      <td>37.538449</td>\n",
       "      <td>1.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>29</td>\n",
       "      <td>0.062343</td>\n",
       "      <td>0.028649</td>\n",
       "      <td>0.365320</td>\n",
       "      <td>11.358583</td>\n",
       "      <td>1.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>30</td>\n",
       "      <td>0.074715</td>\n",
       "      <td>0.034417</td>\n",
       "      <td>0.365554</td>\n",
       "      <td>13.843380</td>\n",
       "      <td>2.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>31</td>\n",
       "      <td>0.100683</td>\n",
       "      <td>0.044083</td>\n",
       "      <td>0.398182</td>\n",
       "      <td>28.171154</td>\n",
       "      <td>2.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>32</td>\n",
       "      <td>0.086316</td>\n",
       "      <td>0.040215</td>\n",
       "      <td>0.469829</td>\n",
       "      <td>22.468761</td>\n",
       "      <td>2.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>33</td>\n",
       "      <td>0.065952</td>\n",
       "      <td>0.031030</td>\n",
       "      <td>0.428958</td>\n",
       "      <td>21.912076</td>\n",
       "      <td>2.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>34</td>\n",
       "      <td>0.080006</td>\n",
       "      <td>0.036600</td>\n",
       "      <td>0.370763</td>\n",
       "      <td>29.169384</td>\n",
       "      <td>2.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>35</td>\n",
       "      <td>0.070319</td>\n",
       "      <td>0.032743</td>\n",
       "      <td>0.496559</td>\n",
       "      <td>27.752928</td>\n",
       "      <td>2.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>36</td>\n",
       "      <td>0.075126</td>\n",
       "      <td>0.034365</td>\n",
       "      <td>0.396775</td>\n",
       "      <td>32.961924</td>\n",
       "      <td>2.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>37</td>\n",
       "      <td>0.069298</td>\n",
       "      <td>0.032485</td>\n",
       "      <td>0.356036</td>\n",
       "      <td>75.856388</td>\n",
       "      <td>2.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>38</td>\n",
       "      <td>0.072247</td>\n",
       "      <td>0.033980</td>\n",
       "      <td>0.491660</td>\n",
       "      <td>46.681851</td>\n",
       "      <td>2.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>39</td>\n",
       "      <td>0.063728</td>\n",
       "      <td>0.029235</td>\n",
       "      <td>0.576162</td>\n",
       "      <td>32.989907</td>\n",
       "      <td>1.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>40</td>\n",
       "      <td>0.072299</td>\n",
       "      <td>0.034301</td>\n",
       "      <td>0.454764</td>\n",
       "      <td>28.183012</td>\n",
       "      <td>2.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>41</td>\n",
       "      <td>0.104869</td>\n",
       "      <td>0.038667</td>\n",
       "      <td>3.955721</td>\n",
       "      <td>26.511180</td>\n",
       "      <td>2.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    model       nll     brier       ece        mce  classification error\n",
       "0      17  0.071337  0.034025  0.623382  76.294943                  2.29\n",
       "1      18  0.069826  0.033097  0.364700  73.533693                  2.19\n",
       "2      19  0.067577  0.033133  0.501464  37.740637                  2.23\n",
       "3      20  0.073305  0.033707  0.347560  70.554627                  2.10\n",
       "4      21  0.071443  0.033369  0.477062  26.227307                  2.26\n",
       "5      22  0.087290  0.039987  0.486031  72.844945                  2.49\n",
       "6      23  0.073361  0.034511  0.287854  11.007471                  2.29\n",
       "7      24  0.069724  0.033737  0.419690  30.359291                  2.20\n",
       "8      25  0.078615  0.036777  0.172535  22.989505                  2.40\n",
       "9      26  0.081919  0.039556  0.418288  28.227966                  2.57\n",
       "10     27  0.079189  0.037731  0.270231  27.209497                  2.42\n",
       "11     28  0.062206  0.029186  0.513560  37.538449                  1.92\n",
       "12     29  0.062343  0.028649  0.365320  11.358583                  1.89\n",
       "13     30  0.074715  0.034417  0.365554  13.843380                  2.21\n",
       "14     31  0.100683  0.044083  0.398182  28.171154                  2.73\n",
       "15     32  0.086316  0.040215  0.469829  22.468761                  2.70\n",
       "16     33  0.065952  0.031030  0.428958  21.912076                  2.09\n",
       "17     34  0.080006  0.036600  0.370763  29.169384                  2.41\n",
       "18     35  0.070319  0.032743  0.496559  27.752928                  2.22\n",
       "19     36  0.075126  0.034365  0.396775  32.961924                  2.26\n",
       "20     37  0.069298  0.032485  0.356036  75.856388                  2.14\n",
       "21     38  0.072247  0.033980  0.491660  46.681851                  2.08\n",
       "22     39  0.063728  0.029235  0.576162  32.989907                  1.87\n",
       "23     40  0.072299  0.034301  0.454764  28.183012                  2.22\n",
       "24     41  0.104869  0.038667  3.955721  26.511180                  2.19"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEMPERATURE SCALING RESULTS (TEST SET)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "NLL\n",
      "0.075 ± 0.01\n",
      "--------------------\n",
      "BRIER\n",
      "0.035 ± 0.00\n",
      "--------------------\n",
      "ECE\n",
      "0.560 ± 0.70\n",
      "--------------------\n",
      "MCE\n",
      "36.496 ± 20.17\n",
      "--------------------\n",
      "CLASSIFICATION ERROR\n",
      "2.255 ± 0.22\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "print('Temperature Scaling Results (test set)'.upper())\n",
    "print('-'*100)\n",
    "metrics = ['nll', 'brier', 'ece', 'mce', 'classification error']\n",
    "for metric in metrics:\n",
    "    print('{0}'.format(metric.upper()))\n",
    "    print('{0:.3f} \\u00B1 {1:.2f}'.format(np.mean(df_ts[metric]), np.std(df_ts[metric])))\n",
    "    print('-'*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPE:\n",
    "    def __init__(self, model, n_ensemble):\n",
    "        self.model = model\n",
    "        self.n_ensemble = n_ensemble\n",
    "        self.original_weights = model.get_weights()\n",
    "        \n",
    "    def perturb(self, sigma):\n",
    "        self.model.set_weights(self.original_weights)\n",
    "        weights = self.model.get_weights()\n",
    "        for index2, w in enumerate(weights):\n",
    "            shape = w.shape\n",
    "            if len(shape) > 1 and shape[1] == 200:\n",
    "                noise = np.random.normal(0, sigma, (w.shape[0], w.shape[1]))\n",
    "                # noise = np.random.normal(0, sigma, (w.shape[0], w.shape[1]//2))\n",
    "                # zero sum perturbations\n",
    "                # noise = np.concatenate([noise, -noise], axis=-1)\n",
    "                weights[index2] = weights[index2] + noise\n",
    "        self.model.set_weights(weights)\n",
    "    \n",
    "    def ensemble_pred(self, x, sigma):\n",
    "        y_pred = self.model.predict(x)\n",
    "        y_pred_ensemble = np.zeros((self.n_ensemble, *y_pred.shape))\n",
    "        for seed_index, seed in enumerate(range(17, 17 + self.n_ensemble)):\n",
    "            np.random.seed(seed)\n",
    "            self.perturb(sigma)\n",
    "            y_pred = self.model.predict(x)\n",
    "            y_pred_ensemble[seed_index] = y_pred\n",
    "        self.model.set_weights(self.original_weights)\n",
    "        return np.mean(y_pred_ensemble, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def golden_search(model, sigma_lower=0, sigma_upper=0.1, n_ppe_ensemble=10, n_iterations=10):\n",
    "    sigmas_log = list()\n",
    "    nlls_log = list()\n",
    "    nlls = dict()\n",
    "    PHI = (np.sqrt(5) - 1) / 2\n",
    "    for iteration in range(n_iterations):\n",
    "        print('iteration {} {} {} '.format(iteration, sigma_lower, sigma_upper) + '-'* 10)\n",
    "        sigma_1 = sigma_lower + (sigma_upper - sigma_lower) * PHI\n",
    "        sigma_2 = sigma_lower + (sigma_1 - sigma_lower) * PHI\n",
    "        sigmas_log.append(OrderedDict({\"iteration\": iteration, \"sigma_l\": sigma_lower,\n",
    "                                       \"sigma_2\": sigma_2, \"sigma_1\": sigma_1,\n",
    "                                       \"sigma_h\": sigma_upper}))\n",
    "        for sigma in [sigma_1, sigma_2]:\n",
    "            if sigma not in nlls.keys():\n",
    "                ppe = PPE(model, n_ensemble=n_ppe_ensemble)\n",
    "                y_val_ppe = ppe.ensemble_pred(x_val, sigma)\n",
    "                nll = log_loss(y_val, y_val_ppe)\n",
    "                nlls[sigma] = nll\n",
    "                print(sigma, nll)\n",
    "                nlls_log.append(OrderedDict({\"iteration\": iteration, \"sigma\": sigma, \"nll\": nll}))\n",
    "            else:\n",
    "                print('will skip for {0:6f}'.format(sigma))\n",
    "        if nlls[sigma_1] < nlls[sigma_2]:\n",
    "            sigma_lower = sigma_2\n",
    "            print('updated lower side...')\n",
    "        else:\n",
    "            sigma_upper = sigma_1\n",
    "            print('updated upper side...')\n",
    "    sigma_optimal = (sigmas_log[-1]['sigma_l'] + sigmas_log[-1]['sigma_h'])/2\n",
    "    return sigma_optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06377674 0.06803399 0.0736068  0.06540287 0.06246118 0.06377674\n",
      " 0.05983006 0.05983006 0.05212862 0.07147817 0.07229124 0.06114562\n",
      " 0.06195868 0.06590537 0.06722093 0.06246118 0.06114562 0.05901699\n",
      " 0.06884705 0.06934955 0.05425725 0.06377674 0.0736068  0.06590537\n",
      " 0.05688837]\n"
     ]
    }
   ],
   "source": [
    "optimal_sigmas = []\n",
    "optimal_sigmas_path = os.path.join(output_baseline_folder, 'golden_optimization_simgas.csv')\n",
    "if not os.path.isfile(optimal_sigmas_path):\n",
    "    for index, model_id in enumerate(range(17, 17 + n_models)):\n",
    "        print('finding optimal sigma for model {}...'.format(model_id))\n",
    "        model_folder = os.path.join(output_baseline_folder, str(model_id))\n",
    "        checkpoint_path = glob.glob( model_folder + '/weights.' + str(epochs).zfill(2) + '*.hdf5')[0]\n",
    "        model = mlp()\n",
    "        print('loading weights...')\n",
    "        model.load_weights(checkpoint_path)\n",
    "        print('running optimization...')\n",
    "        optimal_sigma = golden_search(model)\n",
    "        optimal_sigmas.append(optimal_sigma)\n",
    "    df = pd.DataFrame(optimal_sigmas, columns=['optimal sigma'])\n",
    "    df.to_csv(optimal_sigmas_path, index=False)\n",
    "else:\n",
    "    df = pd.read_csv(optimal_sigmas_path)\n",
    "optimal_sigmas = df['optimal sigma'].values\n",
    "print(optimal_sigmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06377674 0.06803399 0.0736068  0.06540287 0.06246118 0.06377674\n",
      " 0.05983006 0.05983006 0.05212862 0.07147817 0.07229124 0.06114562\n",
      " 0.06195868 0.06590537 0.06722093 0.06246118 0.06114562 0.05901699\n",
      " 0.06884705 0.06934955 0.05425725 0.06377674 0.0736068  0.06590537\n",
      " 0.05688837]\n"
     ]
    }
   ],
   "source": [
    "print(optimal_sigmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ppe_ensemble = 25\n",
    "for index, model_id in enumerate(range(17, 17+n_models)):\n",
    "    model_folder = os.path.join(output_baseline_folder, str(model_id))\n",
    "    output_ppe_folder = os.path.join(model_folder, 'ppe')\n",
    "    if not os.path.isdir(output_ppe_folder):\n",
    "        os.mkdir(output_ppe_folder)\n",
    "    test_pred_path = os.path.join(output_ppe_folder, 'test_pred_ppe.npy')\n",
    "    if not os.path.isfile(test_pred_path):\n",
    "    # if True:\n",
    "        checkpoint_path = glob.glob(model_folder + '/weights.' + str(epochs).zfill(2) + '*.hdf5')[0]\n",
    "        model = mlp()\n",
    "        print('loading weights for model {}...'.format(model_id))\n",
    "        model.load_weights(checkpoint_path)\n",
    "        optimal_sigma = optimal_sigmas[index]\n",
    "        ppe = PPE(model, n_ensemble=n_ppe_ensemble)\n",
    "        y_pred = ppe.ensemble_pred(x_test, optimal_sigma)\n",
    "        np.save(test_pred_path, y_pred)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = list()\n",
    "for index, model_id in enumerate(range(17, 17 + n_models)):\n",
    "    model_ppe_folder= os.path.join(output_baseline_folder, str(model_id),  'ppe')\n",
    "    y_pred = np.load(os.path.join(model_ppe_folder, 'test_pred_ppe.npy'))\n",
    "    nll = log_loss(y_test, y_pred)\n",
    "    error = classification_error(y_test, y_pred)\n",
    "    br = brier_multi(y_test, y_pred)\n",
    "    calib_erros = CalibrationErrors(y_test, y_pred, bin_size=1 / 20., min_samples=0)\n",
    "    _, _, ece, mce, _ = calib_erros.calculate_calibration_errors()\n",
    "    d.append(OrderedDict({\"model\":model_id, \"nll\": nll, \"brier\": br, \"ece\": ece, \"mce\":mce, \"classification error\": error}))\n",
    "df_ppe = pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>nll</th>\n",
       "      <th>brier</th>\n",
       "      <th>ece</th>\n",
       "      <th>mce</th>\n",
       "      <th>classification error</th>\n",
       "      <th>model</th>\n",
       "      <th>nll</th>\n",
       "      <th>brier</th>\n",
       "      <th>ece</th>\n",
       "      <th>mce</th>\n",
       "      <th>classification error</th>\n",
       "      <th>model</th>\n",
       "      <th>nll</th>\n",
       "      <th>brier</th>\n",
       "      <th>ece</th>\n",
       "      <th>mce</th>\n",
       "      <th>classification error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0.085811</td>\n",
       "      <td>0.036027</td>\n",
       "      <td>1.212307</td>\n",
       "      <td>66.113958</td>\n",
       "      <td>2.29</td>\n",
       "      <td>17</td>\n",
       "      <td>0.071723</td>\n",
       "      <td>0.033656</td>\n",
       "      <td>0.513987</td>\n",
       "      <td>38.100467</td>\n",
       "      <td>2.28</td>\n",
       "      <td>17</td>\n",
       "      <td>0.071337</td>\n",
       "      <td>0.034025</td>\n",
       "      <td>0.623382</td>\n",
       "      <td>76.294943</td>\n",
       "      <td>2.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0.088970</td>\n",
       "      <td>0.035416</td>\n",
       "      <td>1.293205</td>\n",
       "      <td>30.072403</td>\n",
       "      <td>2.19</td>\n",
       "      <td>18</td>\n",
       "      <td>0.072249</td>\n",
       "      <td>0.032823</td>\n",
       "      <td>0.338364</td>\n",
       "      <td>33.829188</td>\n",
       "      <td>2.18</td>\n",
       "      <td>18</td>\n",
       "      <td>0.069826</td>\n",
       "      <td>0.033097</td>\n",
       "      <td>0.364700</td>\n",
       "      <td>73.533693</td>\n",
       "      <td>2.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>0.083936</td>\n",
       "      <td>0.035472</td>\n",
       "      <td>1.317934</td>\n",
       "      <td>26.070374</td>\n",
       "      <td>2.23</td>\n",
       "      <td>19</td>\n",
       "      <td>0.069112</td>\n",
       "      <td>0.033780</td>\n",
       "      <td>0.510336</td>\n",
       "      <td>32.528024</td>\n",
       "      <td>2.27</td>\n",
       "      <td>19</td>\n",
       "      <td>0.067577</td>\n",
       "      <td>0.033133</td>\n",
       "      <td>0.501464</td>\n",
       "      <td>37.740637</td>\n",
       "      <td>2.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>0.090023</td>\n",
       "      <td>0.035112</td>\n",
       "      <td>1.139185</td>\n",
       "      <td>37.549862</td>\n",
       "      <td>2.10</td>\n",
       "      <td>20</td>\n",
       "      <td>0.076215</td>\n",
       "      <td>0.034030</td>\n",
       "      <td>0.437748</td>\n",
       "      <td>42.203048</td>\n",
       "      <td>2.18</td>\n",
       "      <td>20</td>\n",
       "      <td>0.073305</td>\n",
       "      <td>0.033707</td>\n",
       "      <td>0.347560</td>\n",
       "      <td>70.554627</td>\n",
       "      <td>2.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>0.092218</td>\n",
       "      <td>0.035476</td>\n",
       "      <td>1.343740</td>\n",
       "      <td>29.371622</td>\n",
       "      <td>2.26</td>\n",
       "      <td>21</td>\n",
       "      <td>0.076591</td>\n",
       "      <td>0.033343</td>\n",
       "      <td>0.528798</td>\n",
       "      <td>29.254233</td>\n",
       "      <td>2.17</td>\n",
       "      <td>21</td>\n",
       "      <td>0.071443</td>\n",
       "      <td>0.033369</td>\n",
       "      <td>0.477062</td>\n",
       "      <td>26.227307</td>\n",
       "      <td>2.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>0.118141</td>\n",
       "      <td>0.042232</td>\n",
       "      <td>1.489946</td>\n",
       "      <td>20.196827</td>\n",
       "      <td>2.49</td>\n",
       "      <td>22</td>\n",
       "      <td>0.096007</td>\n",
       "      <td>0.040888</td>\n",
       "      <td>0.583511</td>\n",
       "      <td>33.871916</td>\n",
       "      <td>2.55</td>\n",
       "      <td>22</td>\n",
       "      <td>0.087290</td>\n",
       "      <td>0.039987</td>\n",
       "      <td>0.486031</td>\n",
       "      <td>72.844945</td>\n",
       "      <td>2.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>0.097130</td>\n",
       "      <td>0.037205</td>\n",
       "      <td>1.411669</td>\n",
       "      <td>39.474791</td>\n",
       "      <td>2.29</td>\n",
       "      <td>23</td>\n",
       "      <td>0.080109</td>\n",
       "      <td>0.035212</td>\n",
       "      <td>0.704254</td>\n",
       "      <td>21.005549</td>\n",
       "      <td>2.37</td>\n",
       "      <td>23</td>\n",
       "      <td>0.073361</td>\n",
       "      <td>0.034511</td>\n",
       "      <td>0.287854</td>\n",
       "      <td>11.007471</td>\n",
       "      <td>2.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>24</td>\n",
       "      <td>0.085837</td>\n",
       "      <td>0.035658</td>\n",
       "      <td>1.225365</td>\n",
       "      <td>37.476841</td>\n",
       "      <td>2.20</td>\n",
       "      <td>24</td>\n",
       "      <td>0.072158</td>\n",
       "      <td>0.033309</td>\n",
       "      <td>0.431632</td>\n",
       "      <td>66.175535</td>\n",
       "      <td>2.17</td>\n",
       "      <td>24</td>\n",
       "      <td>0.069724</td>\n",
       "      <td>0.033737</td>\n",
       "      <td>0.419690</td>\n",
       "      <td>30.359291</td>\n",
       "      <td>2.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>25</td>\n",
       "      <td>0.101171</td>\n",
       "      <td>0.038938</td>\n",
       "      <td>1.412432</td>\n",
       "      <td>38.668972</td>\n",
       "      <td>2.40</td>\n",
       "      <td>25</td>\n",
       "      <td>0.088058</td>\n",
       "      <td>0.037930</td>\n",
       "      <td>0.710200</td>\n",
       "      <td>26.668325</td>\n",
       "      <td>2.48</td>\n",
       "      <td>25</td>\n",
       "      <td>0.078615</td>\n",
       "      <td>0.036777</td>\n",
       "      <td>0.172535</td>\n",
       "      <td>22.989505</td>\n",
       "      <td>2.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>26</td>\n",
       "      <td>0.105199</td>\n",
       "      <td>0.041657</td>\n",
       "      <td>1.461620</td>\n",
       "      <td>19.777432</td>\n",
       "      <td>2.57</td>\n",
       "      <td>26</td>\n",
       "      <td>0.082296</td>\n",
       "      <td>0.038389</td>\n",
       "      <td>0.562042</td>\n",
       "      <td>31.455908</td>\n",
       "      <td>2.39</td>\n",
       "      <td>26</td>\n",
       "      <td>0.081919</td>\n",
       "      <td>0.039556</td>\n",
       "      <td>0.418288</td>\n",
       "      <td>28.227966</td>\n",
       "      <td>2.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>27</td>\n",
       "      <td>0.103741</td>\n",
       "      <td>0.040022</td>\n",
       "      <td>1.449721</td>\n",
       "      <td>29.872349</td>\n",
       "      <td>2.42</td>\n",
       "      <td>27</td>\n",
       "      <td>0.081015</td>\n",
       "      <td>0.037028</td>\n",
       "      <td>0.525215</td>\n",
       "      <td>33.686465</td>\n",
       "      <td>2.32</td>\n",
       "      <td>27</td>\n",
       "      <td>0.079189</td>\n",
       "      <td>0.037731</td>\n",
       "      <td>0.270231</td>\n",
       "      <td>27.209497</td>\n",
       "      <td>2.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>28</td>\n",
       "      <td>0.075225</td>\n",
       "      <td>0.030420</td>\n",
       "      <td>1.017944</td>\n",
       "      <td>62.407127</td>\n",
       "      <td>1.92</td>\n",
       "      <td>28</td>\n",
       "      <td>0.064023</td>\n",
       "      <td>0.029289</td>\n",
       "      <td>0.357698</td>\n",
       "      <td>27.137156</td>\n",
       "      <td>1.98</td>\n",
       "      <td>28</td>\n",
       "      <td>0.062206</td>\n",
       "      <td>0.029186</td>\n",
       "      <td>0.513560</td>\n",
       "      <td>37.538449</td>\n",
       "      <td>1.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>29</td>\n",
       "      <td>0.078692</td>\n",
       "      <td>0.029996</td>\n",
       "      <td>1.035537</td>\n",
       "      <td>32.658288</td>\n",
       "      <td>1.89</td>\n",
       "      <td>29</td>\n",
       "      <td>0.067366</td>\n",
       "      <td>0.028869</td>\n",
       "      <td>0.396085</td>\n",
       "      <td>34.085315</td>\n",
       "      <td>1.86</td>\n",
       "      <td>29</td>\n",
       "      <td>0.062343</td>\n",
       "      <td>0.028649</td>\n",
       "      <td>0.365320</td>\n",
       "      <td>11.358583</td>\n",
       "      <td>1.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>30</td>\n",
       "      <td>0.092800</td>\n",
       "      <td>0.035851</td>\n",
       "      <td>1.259849</td>\n",
       "      <td>57.814842</td>\n",
       "      <td>2.21</td>\n",
       "      <td>30</td>\n",
       "      <td>0.078856</td>\n",
       "      <td>0.034813</td>\n",
       "      <td>0.467118</td>\n",
       "      <td>11.771773</td>\n",
       "      <td>2.23</td>\n",
       "      <td>30</td>\n",
       "      <td>0.074715</td>\n",
       "      <td>0.034417</td>\n",
       "      <td>0.365554</td>\n",
       "      <td>13.843380</td>\n",
       "      <td>2.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>31</td>\n",
       "      <td>0.140643</td>\n",
       "      <td>0.046708</td>\n",
       "      <td>1.713743</td>\n",
       "      <td>39.164832</td>\n",
       "      <td>2.73</td>\n",
       "      <td>31</td>\n",
       "      <td>0.108564</td>\n",
       "      <td>0.045553</td>\n",
       "      <td>0.545643</td>\n",
       "      <td>41.471752</td>\n",
       "      <td>2.99</td>\n",
       "      <td>31</td>\n",
       "      <td>0.100683</td>\n",
       "      <td>0.044083</td>\n",
       "      <td>0.398182</td>\n",
       "      <td>28.171154</td>\n",
       "      <td>2.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>32</td>\n",
       "      <td>0.111696</td>\n",
       "      <td>0.042825</td>\n",
       "      <td>1.552573</td>\n",
       "      <td>32.555023</td>\n",
       "      <td>2.70</td>\n",
       "      <td>32</td>\n",
       "      <td>0.092696</td>\n",
       "      <td>0.041001</td>\n",
       "      <td>0.533984</td>\n",
       "      <td>27.742003</td>\n",
       "      <td>2.71</td>\n",
       "      <td>32</td>\n",
       "      <td>0.086316</td>\n",
       "      <td>0.040215</td>\n",
       "      <td>0.469829</td>\n",
       "      <td>22.468761</td>\n",
       "      <td>2.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>33</td>\n",
       "      <td>0.084994</td>\n",
       "      <td>0.033658</td>\n",
       "      <td>1.320660</td>\n",
       "      <td>42.629236</td>\n",
       "      <td>2.09</td>\n",
       "      <td>33</td>\n",
       "      <td>0.071553</td>\n",
       "      <td>0.031932</td>\n",
       "      <td>0.585114</td>\n",
       "      <td>43.045656</td>\n",
       "      <td>2.18</td>\n",
       "      <td>33</td>\n",
       "      <td>0.065952</td>\n",
       "      <td>0.031030</td>\n",
       "      <td>0.428958</td>\n",
       "      <td>21.912076</td>\n",
       "      <td>2.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>34</td>\n",
       "      <td>0.106997</td>\n",
       "      <td>0.038911</td>\n",
       "      <td>1.432756</td>\n",
       "      <td>37.999850</td>\n",
       "      <td>2.41</td>\n",
       "      <td>34</td>\n",
       "      <td>0.087284</td>\n",
       "      <td>0.037186</td>\n",
       "      <td>0.654176</td>\n",
       "      <td>21.670569</td>\n",
       "      <td>2.42</td>\n",
       "      <td>34</td>\n",
       "      <td>0.080006</td>\n",
       "      <td>0.036600</td>\n",
       "      <td>0.370763</td>\n",
       "      <td>29.169384</td>\n",
       "      <td>2.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>35</td>\n",
       "      <td>0.092093</td>\n",
       "      <td>0.035220</td>\n",
       "      <td>1.396811</td>\n",
       "      <td>60.147494</td>\n",
       "      <td>2.22</td>\n",
       "      <td>35</td>\n",
       "      <td>0.072200</td>\n",
       "      <td>0.031829</td>\n",
       "      <td>0.526922</td>\n",
       "      <td>75.408877</td>\n",
       "      <td>2.18</td>\n",
       "      <td>35</td>\n",
       "      <td>0.070319</td>\n",
       "      <td>0.032743</td>\n",
       "      <td>0.496559</td>\n",
       "      <td>27.752928</td>\n",
       "      <td>2.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>36</td>\n",
       "      <td>0.100618</td>\n",
       "      <td>0.036551</td>\n",
       "      <td>1.348854</td>\n",
       "      <td>24.788817</td>\n",
       "      <td>2.26</td>\n",
       "      <td>36</td>\n",
       "      <td>0.077512</td>\n",
       "      <td>0.033169</td>\n",
       "      <td>0.318657</td>\n",
       "      <td>37.938126</td>\n",
       "      <td>2.14</td>\n",
       "      <td>36</td>\n",
       "      <td>0.075126</td>\n",
       "      <td>0.034365</td>\n",
       "      <td>0.396775</td>\n",
       "      <td>32.961924</td>\n",
       "      <td>2.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>37</td>\n",
       "      <td>0.085100</td>\n",
       "      <td>0.034180</td>\n",
       "      <td>1.181056</td>\n",
       "      <td>70.227861</td>\n",
       "      <td>2.14</td>\n",
       "      <td>37</td>\n",
       "      <td>0.075437</td>\n",
       "      <td>0.033091</td>\n",
       "      <td>0.528620</td>\n",
       "      <td>76.118445</td>\n",
       "      <td>2.14</td>\n",
       "      <td>37</td>\n",
       "      <td>0.069298</td>\n",
       "      <td>0.032485</td>\n",
       "      <td>0.356036</td>\n",
       "      <td>75.856388</td>\n",
       "      <td>2.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>38</td>\n",
       "      <td>0.094349</td>\n",
       "      <td>0.035589</td>\n",
       "      <td>1.241023</td>\n",
       "      <td>29.721172</td>\n",
       "      <td>2.08</td>\n",
       "      <td>38</td>\n",
       "      <td>0.076024</td>\n",
       "      <td>0.033755</td>\n",
       "      <td>0.655124</td>\n",
       "      <td>28.635460</td>\n",
       "      <td>2.11</td>\n",
       "      <td>38</td>\n",
       "      <td>0.072247</td>\n",
       "      <td>0.033980</td>\n",
       "      <td>0.491660</td>\n",
       "      <td>46.681851</td>\n",
       "      <td>2.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>39</td>\n",
       "      <td>0.078986</td>\n",
       "      <td>0.030598</td>\n",
       "      <td>1.043884</td>\n",
       "      <td>43.188384</td>\n",
       "      <td>1.87</td>\n",
       "      <td>39</td>\n",
       "      <td>0.067234</td>\n",
       "      <td>0.029584</td>\n",
       "      <td>0.379844</td>\n",
       "      <td>27.291608</td>\n",
       "      <td>1.93</td>\n",
       "      <td>39</td>\n",
       "      <td>0.063728</td>\n",
       "      <td>0.029235</td>\n",
       "      <td>0.576162</td>\n",
       "      <td>32.989907</td>\n",
       "      <td>1.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>40</td>\n",
       "      <td>0.096331</td>\n",
       "      <td>0.036958</td>\n",
       "      <td>1.397005</td>\n",
       "      <td>68.538457</td>\n",
       "      <td>2.22</td>\n",
       "      <td>40</td>\n",
       "      <td>0.080486</td>\n",
       "      <td>0.035903</td>\n",
       "      <td>0.681229</td>\n",
       "      <td>28.635043</td>\n",
       "      <td>2.33</td>\n",
       "      <td>40</td>\n",
       "      <td>0.072299</td>\n",
       "      <td>0.034301</td>\n",
       "      <td>0.454764</td>\n",
       "      <td>28.183012</td>\n",
       "      <td>2.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>41</td>\n",
       "      <td>0.078084</td>\n",
       "      <td>0.035312</td>\n",
       "      <td>0.848223</td>\n",
       "      <td>32.993051</td>\n",
       "      <td>2.19</td>\n",
       "      <td>41</td>\n",
       "      <td>0.090462</td>\n",
       "      <td>0.040017</td>\n",
       "      <td>0.735083</td>\n",
       "      <td>72.105548</td>\n",
       "      <td>2.60</td>\n",
       "      <td>41</td>\n",
       "      <td>0.104869</td>\n",
       "      <td>0.038667</td>\n",
       "      <td>3.955721</td>\n",
       "      <td>26.511180</td>\n",
       "      <td>2.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    model       nll     brier       ece        mce  classification error  \\\n",
       "0      17  0.085811  0.036027  1.212307  66.113958                  2.29   \n",
       "1      18  0.088970  0.035416  1.293205  30.072403                  2.19   \n",
       "2      19  0.083936  0.035472  1.317934  26.070374                  2.23   \n",
       "3      20  0.090023  0.035112  1.139185  37.549862                  2.10   \n",
       "4      21  0.092218  0.035476  1.343740  29.371622                  2.26   \n",
       "5      22  0.118141  0.042232  1.489946  20.196827                  2.49   \n",
       "6      23  0.097130  0.037205  1.411669  39.474791                  2.29   \n",
       "7      24  0.085837  0.035658  1.225365  37.476841                  2.20   \n",
       "8      25  0.101171  0.038938  1.412432  38.668972                  2.40   \n",
       "9      26  0.105199  0.041657  1.461620  19.777432                  2.57   \n",
       "10     27  0.103741  0.040022  1.449721  29.872349                  2.42   \n",
       "11     28  0.075225  0.030420  1.017944  62.407127                  1.92   \n",
       "12     29  0.078692  0.029996  1.035537  32.658288                  1.89   \n",
       "13     30  0.092800  0.035851  1.259849  57.814842                  2.21   \n",
       "14     31  0.140643  0.046708  1.713743  39.164832                  2.73   \n",
       "15     32  0.111696  0.042825  1.552573  32.555023                  2.70   \n",
       "16     33  0.084994  0.033658  1.320660  42.629236                  2.09   \n",
       "17     34  0.106997  0.038911  1.432756  37.999850                  2.41   \n",
       "18     35  0.092093  0.035220  1.396811  60.147494                  2.22   \n",
       "19     36  0.100618  0.036551  1.348854  24.788817                  2.26   \n",
       "20     37  0.085100  0.034180  1.181056  70.227861                  2.14   \n",
       "21     38  0.094349  0.035589  1.241023  29.721172                  2.08   \n",
       "22     39  0.078986  0.030598  1.043884  43.188384                  1.87   \n",
       "23     40  0.096331  0.036958  1.397005  68.538457                  2.22   \n",
       "24     41  0.078084  0.035312  0.848223  32.993051                  2.19   \n",
       "\n",
       "    model       nll     brier       ece        mce  classification error  \\\n",
       "0      17  0.071723  0.033656  0.513987  38.100467                  2.28   \n",
       "1      18  0.072249  0.032823  0.338364  33.829188                  2.18   \n",
       "2      19  0.069112  0.033780  0.510336  32.528024                  2.27   \n",
       "3      20  0.076215  0.034030  0.437748  42.203048                  2.18   \n",
       "4      21  0.076591  0.033343  0.528798  29.254233                  2.17   \n",
       "5      22  0.096007  0.040888  0.583511  33.871916                  2.55   \n",
       "6      23  0.080109  0.035212  0.704254  21.005549                  2.37   \n",
       "7      24  0.072158  0.033309  0.431632  66.175535                  2.17   \n",
       "8      25  0.088058  0.037930  0.710200  26.668325                  2.48   \n",
       "9      26  0.082296  0.038389  0.562042  31.455908                  2.39   \n",
       "10     27  0.081015  0.037028  0.525215  33.686465                  2.32   \n",
       "11     28  0.064023  0.029289  0.357698  27.137156                  1.98   \n",
       "12     29  0.067366  0.028869  0.396085  34.085315                  1.86   \n",
       "13     30  0.078856  0.034813  0.467118  11.771773                  2.23   \n",
       "14     31  0.108564  0.045553  0.545643  41.471752                  2.99   \n",
       "15     32  0.092696  0.041001  0.533984  27.742003                  2.71   \n",
       "16     33  0.071553  0.031932  0.585114  43.045656                  2.18   \n",
       "17     34  0.087284  0.037186  0.654176  21.670569                  2.42   \n",
       "18     35  0.072200  0.031829  0.526922  75.408877                  2.18   \n",
       "19     36  0.077512  0.033169  0.318657  37.938126                  2.14   \n",
       "20     37  0.075437  0.033091  0.528620  76.118445                  2.14   \n",
       "21     38  0.076024  0.033755  0.655124  28.635460                  2.11   \n",
       "22     39  0.067234  0.029584  0.379844  27.291608                  1.93   \n",
       "23     40  0.080486  0.035903  0.681229  28.635043                  2.33   \n",
       "24     41  0.090462  0.040017  0.735083  72.105548                  2.60   \n",
       "\n",
       "    model       nll     brier       ece        mce  classification error  \n",
       "0      17  0.071337  0.034025  0.623382  76.294943                  2.29  \n",
       "1      18  0.069826  0.033097  0.364700  73.533693                  2.19  \n",
       "2      19  0.067577  0.033133  0.501464  37.740637                  2.23  \n",
       "3      20  0.073305  0.033707  0.347560  70.554627                  2.10  \n",
       "4      21  0.071443  0.033369  0.477062  26.227307                  2.26  \n",
       "5      22  0.087290  0.039987  0.486031  72.844945                  2.49  \n",
       "6      23  0.073361  0.034511  0.287854  11.007471                  2.29  \n",
       "7      24  0.069724  0.033737  0.419690  30.359291                  2.20  \n",
       "8      25  0.078615  0.036777  0.172535  22.989505                  2.40  \n",
       "9      26  0.081919  0.039556  0.418288  28.227966                  2.57  \n",
       "10     27  0.079189  0.037731  0.270231  27.209497                  2.42  \n",
       "11     28  0.062206  0.029186  0.513560  37.538449                  1.92  \n",
       "12     29  0.062343  0.028649  0.365320  11.358583                  1.89  \n",
       "13     30  0.074715  0.034417  0.365554  13.843380                  2.21  \n",
       "14     31  0.100683  0.044083  0.398182  28.171154                  2.73  \n",
       "15     32  0.086316  0.040215  0.469829  22.468761                  2.70  \n",
       "16     33  0.065952  0.031030  0.428958  21.912076                  2.09  \n",
       "17     34  0.080006  0.036600  0.370763  29.169384                  2.41  \n",
       "18     35  0.070319  0.032743  0.496559  27.752928                  2.22  \n",
       "19     36  0.075126  0.034365  0.396775  32.961924                  2.26  \n",
       "20     37  0.069298  0.032485  0.356036  75.856388                  2.14  \n",
       "21     38  0.072247  0.033980  0.491660  46.681851                  2.08  \n",
       "22     39  0.063728  0.029235  0.576162  32.989907                  1.87  \n",
       "23     40  0.072299  0.034301  0.454764  28.183012                  2.22  \n",
       "24     41  0.104869  0.038667  3.955721  26.511180                  2.19  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([df_baselines, df_ppe, df_ts], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPE RESULTS (TEST SET)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "NLL\n",
      "0.079 ± 0.0101\n",
      "--------------------\n",
      "BRIER\n",
      "0.035 ± 0.0039\n",
      "--------------------\n",
      "ECE\n",
      "0.528 ± 0.1167\n",
      "--------------------\n",
      "MCE\n",
      "37.673 ± 16.6910\n",
      "--------------------\n",
      "CLASSIFICATION ERROR\n",
      "2.286 ± 0.2426\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "print('PPE Results (test set)'.upper())\n",
    "print('-'*100)\n",
    "metrics = ['nll', 'brier', 'ece', 'mce', 'classification error']\n",
    "for metric in metrics:\n",
    "    print('{0}'.format(metric.upper()))\n",
    "    print('{0:.3f} \\u00B1 {1:.4f}'.format(np.mean(df_ppe[metric]), np.std(df_ppe[metric])))\n",
    "    print('-'*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MC Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_mcdo(num_classes=10, dropout=True, level=0.5):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(200, activation='relu', input_shape=(784,)))\n",
    "    model.add(BatchNormalization())\n",
    "    if dropout:\n",
    "        model.add(Lambda(lambda x: K.dropout(x, level=level)))\n",
    "    model.add(Dense(200, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    if dropout:\n",
    "        model.add(Lambda(lambda x: K.dropout(x, level=level)))\n",
    "    model.add(Dense(200, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    if dropout:\n",
    "        model.add(Lambda(lambda x: K.dropout(x, level=level)))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 2.5e-3\n",
    "#\n",
    "output_baseline_dropout_folder = os.path.join(output_folder, 'baselines_dropout')\n",
    "if not os.path.isdir(output_baseline_dropout_folder):\n",
    "    os.makedirs(output_baseline_dropout_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(n_models):\n",
    "    seed = i+17\n",
    "    # change the random state\n",
    "    np.random.seed(seed)\n",
    "    # creating output folder for each of the baseline models\n",
    "    output_model_folder = os.path.join(output_baseline_dropout_folder, str(seed))\n",
    "    if not os.path.isdir(output_model_folder):\n",
    "        os.mkdir(output_model_folder)\n",
    "    else:\n",
    "        continue\n",
    "    # callbacks for training\n",
    "    callbacks = list()\n",
    "    log = CSVLogger(os.path.join(output_model_folder, 'log.csv'))\n",
    "    callbacks.append(log)\n",
    "    checkpoint_path = os.path.join(output_model_folder, 'weights.{epoch:02d}.hdf5')\n",
    "    checkpoint = ModelCheckpoint(checkpoint_path, period=1, save_best_only=False, verbose=True)\n",
    "    callbacks.append(checkpoint)\n",
    "    # initialize and compile the model\n",
    "    model = mlp_mcdo()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=learning_rate), metrics=['accuracy'])\n",
    "    # train\n",
    "    model.fit(x_train, y_train, batch_size=128, epochs=15, verbose=1, callbacks=callbacks, validation_data=(x_val, y_val), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(17, 17+n_models):\n",
    "    log = pd.read_csv(os.path.join(output_baseline_dropout_folder, str(i), 'log.csv'))\n",
    "    plt.figure(figsize=(8,2),dpi=100)\n",
    "    plt.subplots_adjust(wspace=0.5)\n",
    "    ax1 = plt.subplot(121)\n",
    "    ax2 = plt.subplot(122)\n",
    "    for metric_type in ['', 'val_']:\n",
    "        for key in log.keys():\n",
    "            if 'loss' in key and 'val' not in key:\n",
    "                p = ax1.plot(log[metric_type+ key], label=metric_type + key+str(i), linewidth=1)\n",
    "                c = p[-1].get_color()\n",
    "                ax1.plot(np.argmin(log[metric_type+key]), np.amin(log[metric_type+key]), \n",
    "                         '*', markersize=10, alpha=0.5, color = c)\n",
    "                ax1.set_title('min val NLL: {0:.3f}'.format(np.amin(log['val_' + key])))\n",
    "                ax1.set_ylabel('NLL')\n",
    "                ax1.set_xlabel('epochs')\n",
    "            if 'acc' in key and 'val' not in key:\n",
    "                p = ax2.plot(log[metric_type + key], label=metric_type + key+str(i), linewidth=1)\n",
    "                c = p[-1].get_color()\n",
    "                ax2.plot(np.argmax(log[metric_type + key]), np.amax(log[metric_type + key]), \n",
    "                         '*', markersize=10, alpha=0.5, color=c)\n",
    "                ax2.set_title('max val acc: {0:.2f}'.format(np.amax(log['val_' + key]*100)))\n",
    "                ax2.set_ylabel('accuracy')\n",
    "                ax2.set_xlabel('epochs')\n",
    "    ax1.grid()\n",
    "    ax2.grid()\n",
    "    ax1.legend()\n",
    "    ax2.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, model_id in enumerate(range(17, 17+n_models)):\n",
    "    test_pred_path = os.path.join(model_folder, 'test_pred.npy')\n",
    "    if not os.path.isfile(test_pred_path):\n",
    "    # if True:\n",
    "        model_folder = os.path.join(output_baseline_dropout_folder, str(model_id))\n",
    "        checkpoint_path = glob.glob(model_folder + '/weights.' + str(epochs).zfill(2) + '*.hdf5')[0]\n",
    "        model = mlp_mcdo(dropout=False)\n",
    "        print('loading weights for model {}...'.format(model_id))\n",
    "        model.load_weights(checkpoint_path)\n",
    "        print('run inference...')\n",
    "        test_pred = model.predict(x_test)\n",
    "        np.save(test_pred_path, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_model_id = 17\n",
    "y_pred_sample = np.load(os.path.join(output_baseline_folder, str(sample_model_id), 'test_pred.npy'))\n",
    "shape = np.shape(y_pred_sample)\n",
    "all_test_preds = np.zeros((n_models, *shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = []\n",
    "for index, model_id in enumerate(range(17, 17 + n_models)):\n",
    "    y_pred = np.load(os.path.join(output_baseline_dropout_folder, str(model_id), 'test_pred.npy'))\n",
    "    all_test_preds[index] = y_pred\n",
    "    nll = log_loss(y_test, y_pred)\n",
    "    error = classification_error(y_test, y_pred)\n",
    "    br = brier_multi(y_test, y_pred)\n",
    "    calib_erros = CalibrationErrors(y_test, y_pred, bin_size=1 / 20., min_samples=0)\n",
    "    _, _, ece, mce, _ = calib_erros.calculate_calibration_errors()\n",
    "    d.append(OrderedDict({\"model\":model_id, \"nll\": nll, \"brier\": br, \"ece\": ece, \"mce\":mce, \"classification error\": error}))\n",
    "df_baselines_do = pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASELINE DROPOUT RESULTS (TEST SET)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "NLL\n",
      "0.079 ± 0.00\n",
      "--------------------\n",
      "BRIER\n",
      "0.036 ± 0.00\n",
      "--------------------\n",
      "ECE\n",
      "0.902 ± 0.11\n",
      "--------------------\n",
      "MCE\n",
      "39.309 ± 17.86\n",
      "--------------------\n",
      "CLASSIFICATION ERROR\n",
      "2.320 ± 0.14\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "print('Baseline Dropout Results (test set)'.upper())\n",
    "print('-'*100)\n",
    "metrics = ['nll', 'brier', 'ece', 'mce', 'classification error']\n",
    "for metric in metrics:\n",
    "    print('{0}'.format(metric.upper()))\n",
    "    print('{0:.3f} \\u00B1 {1:.2f}'.format(np.mean(df_baselines_do[metric]), np.std(df_baselines_do[metric])))\n",
    "    print('-'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mcdo_ensemble = 25\n",
    "for index, model_id in enumerate(range(17, 17+n_models)):\n",
    "    model_folder = os.path.join(output_baseline_dropout_folder, str(model_id))\n",
    "    output_mcdo_folder = os.path.join(model_folder, 'mcdo')\n",
    "    if not os.path.isdir(output_mcdo_folder):\n",
    "        os.mkdir(output_mcdo_folder)\n",
    "    test_pred_path = os.path.join(output_mcdo_folder, 'test_pred_mcdo.npy')\n",
    "    if not os.path.isfile(test_pred_path):\n",
    "    #if True:\n",
    "        checkpoint_path = glob.glob(model_folder + '/weights.' + str(epochs).zfill(2) + '*.hdf5')[0]\n",
    "        model = mlp_mcdo()\n",
    "        print('loading weights for model {}...'.format(model_id))\n",
    "        model.load_weights(checkpoint_path)\n",
    "        y_test_pred_ensemble = np.zeros((n_mcdo_ensemble, *y_test.shape))\n",
    "        for index2, seed in enumerate(range(17, 17 + n_mcdo_ensemble)):\n",
    "            np.random.seed(seed)\n",
    "            y_test_pred_dropout = model.predict(x_test, verbose=1)\n",
    "            y_test_pred_ensemble[index2] = y_test_pred_dropout\n",
    "        y_pred = np.mean(y_test_pred_ensemble, axis=0)\n",
    "        np.save(test_pred_path, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = list()\n",
    "for index, model_id in enumerate(range(17, 17 + n_models)):\n",
    "    model_mcdo_folder= os.path.join(output_baseline_dropout_folder, str(model_id),  'mcdo')\n",
    "    y_pred = np.load(os.path.join(model_mcdo_folder, 'test_pred_mcdo.npy'))\n",
    "    nll = log_loss(y_test, y_pred)\n",
    "    error = classification_error(y_test, y_pred)\n",
    "    br = brier_multi(y_test, y_pred)\n",
    "    calib_erros = CalibrationErrors(y_test, y_pred, bin_size=1 / 20., min_samples=0)\n",
    "    _, _, ece, mce, _ = calib_erros.calculate_calibration_errors()\n",
    "    d.append(OrderedDict({\"model\":model_id, \"nll\": nll, \"brier\": br, \"ece\": ece, \"mce\":mce, \"classification error\": error}))\n",
    "df_mcdo= pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>nll</th>\n",
       "      <th>brier</th>\n",
       "      <th>ece</th>\n",
       "      <th>mce</th>\n",
       "      <th>classification error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0.095401</td>\n",
       "      <td>0.040128</td>\n",
       "      <td>2.802617</td>\n",
       "      <td>15.995721</td>\n",
       "      <td>2.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0.096048</td>\n",
       "      <td>0.040929</td>\n",
       "      <td>2.691462</td>\n",
       "      <td>75.973520</td>\n",
       "      <td>2.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>0.096948</td>\n",
       "      <td>0.042116</td>\n",
       "      <td>2.527672</td>\n",
       "      <td>13.857075</td>\n",
       "      <td>2.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>0.100229</td>\n",
       "      <td>0.043558</td>\n",
       "      <td>2.595171</td>\n",
       "      <td>29.907060</td>\n",
       "      <td>2.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>0.091891</td>\n",
       "      <td>0.039613</td>\n",
       "      <td>2.581485</td>\n",
       "      <td>21.818130</td>\n",
       "      <td>2.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>0.095033</td>\n",
       "      <td>0.040920</td>\n",
       "      <td>2.577780</td>\n",
       "      <td>24.434110</td>\n",
       "      <td>2.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>0.092827</td>\n",
       "      <td>0.041046</td>\n",
       "      <td>2.294056</td>\n",
       "      <td>76.340237</td>\n",
       "      <td>2.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>24</td>\n",
       "      <td>0.094471</td>\n",
       "      <td>0.041704</td>\n",
       "      <td>2.420385</td>\n",
       "      <td>22.781254</td>\n",
       "      <td>2.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>25</td>\n",
       "      <td>0.095666</td>\n",
       "      <td>0.040994</td>\n",
       "      <td>2.864490</td>\n",
       "      <td>43.019410</td>\n",
       "      <td>2.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>26</td>\n",
       "      <td>0.085858</td>\n",
       "      <td>0.037246</td>\n",
       "      <td>2.669731</td>\n",
       "      <td>76.141602</td>\n",
       "      <td>2.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>27</td>\n",
       "      <td>0.090105</td>\n",
       "      <td>0.038870</td>\n",
       "      <td>2.688669</td>\n",
       "      <td>26.874033</td>\n",
       "      <td>2.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>28</td>\n",
       "      <td>0.094789</td>\n",
       "      <td>0.041464</td>\n",
       "      <td>2.562999</td>\n",
       "      <td>26.711414</td>\n",
       "      <td>2.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>29</td>\n",
       "      <td>0.092613</td>\n",
       "      <td>0.040692</td>\n",
       "      <td>2.406405</td>\n",
       "      <td>24.247405</td>\n",
       "      <td>2.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>30</td>\n",
       "      <td>0.097022</td>\n",
       "      <td>0.041790</td>\n",
       "      <td>2.378210</td>\n",
       "      <td>16.976272</td>\n",
       "      <td>2.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>31</td>\n",
       "      <td>0.093204</td>\n",
       "      <td>0.040583</td>\n",
       "      <td>2.325236</td>\n",
       "      <td>21.743650</td>\n",
       "      <td>2.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>32</td>\n",
       "      <td>0.089145</td>\n",
       "      <td>0.038305</td>\n",
       "      <td>2.474134</td>\n",
       "      <td>22.652793</td>\n",
       "      <td>2.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>33</td>\n",
       "      <td>0.092580</td>\n",
       "      <td>0.040078</td>\n",
       "      <td>2.387247</td>\n",
       "      <td>76.140343</td>\n",
       "      <td>2.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>34</td>\n",
       "      <td>0.097986</td>\n",
       "      <td>0.041923</td>\n",
       "      <td>2.753639</td>\n",
       "      <td>28.109172</td>\n",
       "      <td>2.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>35</td>\n",
       "      <td>0.094960</td>\n",
       "      <td>0.040767</td>\n",
       "      <td>2.639249</td>\n",
       "      <td>70.330305</td>\n",
       "      <td>2.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>36</td>\n",
       "      <td>0.088796</td>\n",
       "      <td>0.037873</td>\n",
       "      <td>2.642885</td>\n",
       "      <td>38.955241</td>\n",
       "      <td>2.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>37</td>\n",
       "      <td>0.095545</td>\n",
       "      <td>0.040914</td>\n",
       "      <td>2.552695</td>\n",
       "      <td>24.362984</td>\n",
       "      <td>2.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>38</td>\n",
       "      <td>0.100495</td>\n",
       "      <td>0.042518</td>\n",
       "      <td>2.934879</td>\n",
       "      <td>21.270848</td>\n",
       "      <td>2.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>39</td>\n",
       "      <td>0.095282</td>\n",
       "      <td>0.040974</td>\n",
       "      <td>2.623198</td>\n",
       "      <td>75.877510</td>\n",
       "      <td>2.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>40</td>\n",
       "      <td>0.090076</td>\n",
       "      <td>0.038874</td>\n",
       "      <td>2.318734</td>\n",
       "      <td>21.744968</td>\n",
       "      <td>2.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>41</td>\n",
       "      <td>0.087516</td>\n",
       "      <td>0.037592</td>\n",
       "      <td>2.512833</td>\n",
       "      <td>38.642305</td>\n",
       "      <td>2.21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    model       nll     brier       ece        mce  classification error\n",
       "0      17  0.095401  0.040128  2.802617  15.995721                  2.45\n",
       "1      18  0.096048  0.040929  2.691462  75.973520                  2.42\n",
       "2      19  0.096948  0.042116  2.527672  13.857075                  2.60\n",
       "3      20  0.100229  0.043558  2.595171  29.907060                  2.61\n",
       "4      21  0.091891  0.039613  2.581485  21.818130                  2.37\n",
       "5      22  0.095033  0.040920  2.577780  24.434110                  2.40\n",
       "6      23  0.092827  0.041046  2.294056  76.340237                  2.69\n",
       "7      24  0.094471  0.041704  2.420385  22.781254                  2.59\n",
       "8      25  0.095666  0.040994  2.864490  43.019410                  2.52\n",
       "9      26  0.085858  0.037246  2.669731  76.141602                  2.22\n",
       "10     27  0.090105  0.038870  2.688669  26.874033                  2.28\n",
       "11     28  0.094789  0.041464  2.562999  26.711414                  2.56\n",
       "12     29  0.092613  0.040692  2.406405  24.247405                  2.52\n",
       "13     30  0.097022  0.041790  2.378210  16.976272                  2.66\n",
       "14     31  0.093204  0.040583  2.325236  21.743650                  2.53\n",
       "15     32  0.089145  0.038305  2.474134  22.652793                  2.33\n",
       "16     33  0.092580  0.040078  2.387247  76.140343                  2.42\n",
       "17     34  0.097986  0.041923  2.753639  28.109172                  2.43\n",
       "18     35  0.094960  0.040767  2.639249  70.330305                  2.43\n",
       "19     36  0.088796  0.037873  2.642885  38.955241                  2.22\n",
       "20     37  0.095545  0.040914  2.552695  24.362984                  2.42\n",
       "21     38  0.100495  0.042518  2.934879  21.270848                  2.65\n",
       "22     39  0.095282  0.040974  2.623198  75.877510                  2.44\n",
       "23     40  0.090076  0.038874  2.318734  21.744968                  2.34\n",
       "24     41  0.087516  0.037592  2.512833  38.642305                  2.21"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mcdo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCDO RESULTS (TEST SET)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "NLL\n",
      "0.094 ± 0.00\n",
      "--------------------\n",
      "BRIER\n",
      "0.040 ± 0.00\n",
      "--------------------\n",
      "ECE\n",
      "2.569 ± 0.17\n",
      "--------------------\n",
      "MCE\n",
      "37.396 ± 22.22\n",
      "--------------------\n",
      "CLASSIFICATION ERROR\n",
      "2.452 ± 0.14\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "print('MCDO Results (test set)'.upper())\n",
    "print('-'*100)\n",
    "metrics = ['nll', 'brier', 'ece', 'mce', 'classification error']\n",
    "for metric in metrics:\n",
    "    print('{0}'.format(metric.upper()))\n",
    "    print('{0:.3f} \\u00B1 {1:.2f}'.format(np.mean(df_mcdo[metric]), np.std(df_mcdo[metric])))\n",
    "    print('-'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [df_baselines,\n",
    "df_ppe,\n",
    "df_ts,\n",
    "df_mcdo,\n",
    "df_deep_ensembles,\n",
    "      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% nll\n",
      "0.095 $\\pm$ 0.01 &\n",
      "0.079 $\\pm$ 0.01 &\n",
      "0.075 $\\pm$ 0.01 &\n",
      "0.094 $\\pm$ 0.00 &\n",
      "0.044 $\\pm$ 0.00 \\\\\n",
      "----------------------------------------------------------------------------------------------------\n",
      "% brier\n",
      "0.037 $\\pm$ 0.00 &\n",
      "0.035 $\\pm$ 0.00 &\n",
      "0.035 $\\pm$ 0.00 &\n",
      "0.040 $\\pm$ 0.00 &\n",
      "0.020 $\\pm$ 0.00 \\\\\n",
      "----------------------------------------------------------------------------------------------------\n",
      "% ece\n",
      "1.302 $\\pm$ 0.19 &\n",
      "0.528 $\\pm$ 0.12 &\n",
      "0.560 $\\pm$ 0.70 &\n",
      "2.569 $\\pm$ 0.17 &\n",
      "0.848 $\\pm$ 0.08 \\\\\n",
      "----------------------------------------------------------------------------------------------------\n",
      "% mce\n",
      "40.379 $\\pm$ 14.80 &\n",
      "37.673 $\\pm$ 16.69 &\n",
      "36.496 $\\pm$ 20.17 &\n",
      "37.396 $\\pm$ 22.22 &\n",
      "35.646 $\\pm$ 17.16 \\\\\n",
      "----------------------------------------------------------------------------------------------------\n",
      "% classification error\n",
      "2.255 $\\pm$ 0.22 &\n",
      "2.286 $\\pm$ 0.24 &\n",
      "2.255 $\\pm$ 0.22 &\n",
      "2.452 $\\pm$ 0.14 &\n",
      "1.294 $\\pm$ 0.05 \\\\\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "metrics = ['nll', 'brier', 'ece', 'mce', 'classification error']\n",
    "for metric in metrics:\n",
    "    print('% {}'.format(metric))\n",
    "    for index, df in enumerate(dfs):\n",
    "        if index != len(dfs)-1:\n",
    "            print('{0:.3f} $\\pm$ {1:.2f} &'.format(np.mean(df[metric]), np.std(df[metric])))\n",
    "        else:\n",
    "            print('{0:.3f} $\\pm$ {1:.2f} \\\\\\\\'.format(np.mean(df[metric]), np.std(df[metric])))\n",
    "    print('-'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
