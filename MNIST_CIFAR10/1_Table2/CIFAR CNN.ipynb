{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../..')\n",
    "from helpers.metrics import brier_multi, CalibrationErrors, classification_error\n",
    "from helpers.settings import models_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def load_cifar():\n",
    "    num_classes = 10\n",
    "    (x_train_val, y_train_val), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "    x_train_val = x_train_val.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "\n",
    "    x_train_val /= 255\n",
    "    x_test /= 255\n",
    "\n",
    "    y_train_val = to_categorical(y_train_val, num_classes)\n",
    "    y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "    x_train = x_train_val[:45000]\n",
    "    y_train = y_train_val[:45000]\n",
    "    x_val = x_train_val[45000:]\n",
    "    y_val = y_train_val[45000:]\n",
    "    return (x_train, y_train), (x_val, y_val), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_val, y_val), (x_test, y_test) = load_cifar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "* The model is similar to the one that was used in Deep Ensembles and XXX paper\n",
    "* TODO: try a version of the model without batch normalization and study the effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization\n",
    "from keras.layers.core import Lambda\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Activation, Dropout\n",
    "\n",
    "from keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn():\n",
    "    img_rows, img_cols = 32, 32\n",
    "    input_shape = (img_rows, img_cols, 3)\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(16, (3, 3), padding='same', input_shape=input_shape, activation='relu'))\n",
    "    model.add(BatchNormalization(momentum=.9))\n",
    "    \n",
    "    model.add(Conv2D(16, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization(momentum=.9))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization(momentum=.9))\n",
    "    \n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization(momentum=.9))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(BatchNormalization(momentum=.9))\n",
    "    \n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_models = 25\n",
    "epochs = 15\n",
    "learning_rate = 2.5e-4\n",
    "output_folder = os.path.join(models_folder, 'cifar10_cnn')\n",
    "if not os.path.isdir(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "#\n",
    "output_baseline_folder = os.path.join(output_folder, 'baselines')\n",
    "if not os.path.isdir(output_baseline_folder):\n",
    "    os.makedirs(output_baseline_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip training...\n",
      "skip training...\n",
      "skip training...\n",
      "skip training...\n",
      "skip training...\n",
      "skip training...\n",
      "skip training...\n",
      "skip training...\n",
      "skip training...\n",
      "skip training...\n",
      "skip training...\n",
      "skip training...\n",
      "skip training...\n",
      "skip training...\n",
      "skip training...\n",
      "skip training...\n",
      "skip training...\n",
      "skip training...\n",
      "skip training...\n",
      "skip training...\n",
      "skip training...\n",
      "skip training...\n",
      "skip training...\n",
      "skip training...\n",
      "skip training...\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_models):\n",
    "    seed = i+17\n",
    "    # change the random state\n",
    "    np.random.seed(seed)\n",
    "    # creating output folder for each of the baseline models\n",
    "    output_model_folder = os.path.join(output_baseline_folder, str(seed))\n",
    "    if not os.path.isdir(output_model_folder):\n",
    "        os.mkdir(output_model_folder)\n",
    "    else:\n",
    "        print('skip training...')\n",
    "        continue\n",
    "    # callbacks for training\n",
    "    callbacks = list()\n",
    "    log = CSVLogger(os.path.join(output_model_folder, 'log.csv'))\n",
    "    callbacks.append(log)\n",
    "    checkpoint_path = os.path.join(output_model_folder, 'weights.{epoch:02d}.hdf5')\n",
    "    checkpoint = ModelCheckpoint(checkpoint_path, period=1, save_best_only=False, verbose=True)\n",
    "    callbacks.append(checkpoint)\n",
    "    # initialize and compile the model\n",
    "    model = cnn()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=learning_rate), metrics=['accuracy'])\n",
    "    # train\n",
    "    model.fit(x_train, y_train, batch_size=128, epochs=epochs, verbose=1, callbacks=callbacks, validation_data=(x_val, y_val), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(17, 17+n_models):\n",
    "    log = pd.read_csv(os.path.join(output_baseline_folder, str(i), 'log.csv'))\n",
    "    plt.figure(figsize=(8,2),dpi=100)\n",
    "    plt.subplots_adjust(wspace=0.5)\n",
    "    ax1 = plt.subplot(121)\n",
    "    ax2 = plt.subplot(122)\n",
    "    for metric_type in ['', 'val_']:\n",
    "        for key in log.keys():\n",
    "            if 'loss' in key and 'val' not in key:\n",
    "                p = ax1.plot(log[metric_type+ key], label=metric_type + key+str(i), linewidth=1)\n",
    "                c = p[-1].get_color()\n",
    "                ax1.plot(np.argmin(log[metric_type+key]), np.amin(log[metric_type+key]), \n",
    "                         '*', markersize=10, alpha=0.5, color = c)\n",
    "                ax1.set_title('min val NLL: {0:.3f}'.format(np.amin(log['val_' + key])))\n",
    "                ax1.set_ylabel('NLL')\n",
    "                ax1.set_xlabel('epochs')\n",
    "            if 'acc' in key and 'val' not in key:\n",
    "                p = ax2.plot(log[metric_type + key], label=metric_type + key+str(i), linewidth=1)\n",
    "                c = p[-1].get_color()\n",
    "                ax2.plot(np.argmax(log[metric_type + key]), np.amax(log[metric_type + key]), \n",
    "                         '*', markersize=10, alpha=0.5, color=c)\n",
    "                ax2.set_title('max val acc: {0:.2f}'.format(np.amax(log['val_' + key]*100)))\n",
    "                ax2.set_ylabel('accuracy')\n",
    "                ax2.set_xlabel('epochs')\n",
    "    ax1.grid()\n",
    "    ax2.grid()\n",
    "    ax1.legend()\n",
    "    ax2.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, model_id in enumerate(range(17, 17+n_models)):\n",
    "    model_folder = os.path.join(output_baseline_folder, str(model_id))\n",
    "    test_pred_path = os.path.join(model_folder, 'test_pred.npy')\n",
    "    if not os.path.isfile(test_pred_path):\n",
    "    # if True:\n",
    "        model_folder = os.path.join(output_baseline_folder, str(model_id))\n",
    "        checkpoint_path = glob.glob(model_folder + '/weights.' + str(epochs).zfill(2) + '*.hdf5')[0]\n",
    "        # checkpoint_path = glob.glob(model_folder + '/weights.' + str(best_epochs[index]).zfill(2) + '*.hdf5')[0]\n",
    "        model = cnn()\n",
    "        print('loading weights for model {}...'.format(model_id))\n",
    "        model.load_weights(checkpoint_path)\n",
    "        print('inference...')\n",
    "        test_pred = model.predict(x_test)\n",
    "        np.save(test_pred_path, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_model_id = 17\n",
    "y_pred_sample = np.load(os.path.join(output_baseline_folder, str(sample_model_id), 'test_pred.npy'))\n",
    "shape = np.shape(y_pred_sample)\n",
    "all_test_preds = np.zeros((n_models, *shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = []\n",
    "for index, model_id in enumerate(range(17, 17+n_models)):\n",
    "    y_pred = np.load(os.path.join(output_baseline_folder, str(model_id), 'test_pred.npy'))\n",
    "    all_test_preds[index] = y_pred\n",
    "    nll = log_loss(y_test, y_pred)\n",
    "    error = classification_error(y_test, y_pred)\n",
    "    br = brier_multi(y_test, y_pred)\n",
    "    calib_erros = CalibrationErrors(y_test, y_pred, bin_size=1 / 20., min_samples=0)\n",
    "    _, _, ece, mce, _ = calib_erros.calculate_calibration_errors()\n",
    "    d.append(OrderedDict({\"model\":model_id, \"nll\": nll, \"brier\": br, \"ece\": ece, \"mce\":mce, \"classification error\": error}))\n",
    "df_baselines = pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>nll</th>\n",
       "      <th>brier</th>\n",
       "      <th>ece</th>\n",
       "      <th>mce</th>\n",
       "      <th>classification error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>1.049376</td>\n",
       "      <td>0.467641</td>\n",
       "      <td>11.006592</td>\n",
       "      <td>17.914299</td>\n",
       "      <td>32.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>1.120071</td>\n",
       "      <td>0.492364</td>\n",
       "      <td>12.722073</td>\n",
       "      <td>22.916308</td>\n",
       "      <td>34.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>1.081817</td>\n",
       "      <td>0.472275</td>\n",
       "      <td>10.947023</td>\n",
       "      <td>19.122961</td>\n",
       "      <td>33.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>1.035384</td>\n",
       "      <td>0.459745</td>\n",
       "      <td>10.809647</td>\n",
       "      <td>19.084227</td>\n",
       "      <td>32.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>1.129620</td>\n",
       "      <td>0.488162</td>\n",
       "      <td>13.275932</td>\n",
       "      <td>26.464154</td>\n",
       "      <td>34.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>1.046511</td>\n",
       "      <td>0.461049</td>\n",
       "      <td>12.011800</td>\n",
       "      <td>19.272041</td>\n",
       "      <td>32.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>1.049194</td>\n",
       "      <td>0.463736</td>\n",
       "      <td>11.148562</td>\n",
       "      <td>19.476030</td>\n",
       "      <td>32.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>24</td>\n",
       "      <td>1.030083</td>\n",
       "      <td>0.459401</td>\n",
       "      <td>10.782473</td>\n",
       "      <td>18.340129</td>\n",
       "      <td>32.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>25</td>\n",
       "      <td>1.054359</td>\n",
       "      <td>0.465495</td>\n",
       "      <td>12.315030</td>\n",
       "      <td>20.277822</td>\n",
       "      <td>32.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>26</td>\n",
       "      <td>1.080598</td>\n",
       "      <td>0.472686</td>\n",
       "      <td>11.697054</td>\n",
       "      <td>18.425075</td>\n",
       "      <td>33.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>27</td>\n",
       "      <td>1.029928</td>\n",
       "      <td>0.455730</td>\n",
       "      <td>11.239438</td>\n",
       "      <td>19.754712</td>\n",
       "      <td>31.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>28</td>\n",
       "      <td>1.055803</td>\n",
       "      <td>0.464551</td>\n",
       "      <td>11.917404</td>\n",
       "      <td>19.966303</td>\n",
       "      <td>32.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>29</td>\n",
       "      <td>1.034507</td>\n",
       "      <td>0.453163</td>\n",
       "      <td>11.403457</td>\n",
       "      <td>23.194063</td>\n",
       "      <td>31.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>30</td>\n",
       "      <td>1.059232</td>\n",
       "      <td>0.463348</td>\n",
       "      <td>11.684049</td>\n",
       "      <td>80.103070</td>\n",
       "      <td>32.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>31</td>\n",
       "      <td>1.102552</td>\n",
       "      <td>0.484832</td>\n",
       "      <td>12.436964</td>\n",
       "      <td>22.873992</td>\n",
       "      <td>34.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>32</td>\n",
       "      <td>1.056176</td>\n",
       "      <td>0.465262</td>\n",
       "      <td>11.579560</td>\n",
       "      <td>19.582395</td>\n",
       "      <td>32.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>33</td>\n",
       "      <td>1.100861</td>\n",
       "      <td>0.478181</td>\n",
       "      <td>13.072742</td>\n",
       "      <td>24.500363</td>\n",
       "      <td>33.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>34</td>\n",
       "      <td>1.039079</td>\n",
       "      <td>0.464342</td>\n",
       "      <td>11.304843</td>\n",
       "      <td>20.091574</td>\n",
       "      <td>33.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>35</td>\n",
       "      <td>1.077137</td>\n",
       "      <td>0.480872</td>\n",
       "      <td>12.100875</td>\n",
       "      <td>20.664407</td>\n",
       "      <td>33.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>36</td>\n",
       "      <td>1.041124</td>\n",
       "      <td>0.463309</td>\n",
       "      <td>11.243741</td>\n",
       "      <td>20.097279</td>\n",
       "      <td>32.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>37</td>\n",
       "      <td>1.072076</td>\n",
       "      <td>0.476303</td>\n",
       "      <td>12.533180</td>\n",
       "      <td>20.652961</td>\n",
       "      <td>33.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>38</td>\n",
       "      <td>1.064487</td>\n",
       "      <td>0.472570</td>\n",
       "      <td>11.019156</td>\n",
       "      <td>20.153915</td>\n",
       "      <td>33.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>39</td>\n",
       "      <td>1.045349</td>\n",
       "      <td>0.464439</td>\n",
       "      <td>11.604372</td>\n",
       "      <td>18.856452</td>\n",
       "      <td>32.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>40</td>\n",
       "      <td>1.074239</td>\n",
       "      <td>0.468064</td>\n",
       "      <td>12.351936</td>\n",
       "      <td>21.197368</td>\n",
       "      <td>32.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>41</td>\n",
       "      <td>1.047256</td>\n",
       "      <td>0.468077</td>\n",
       "      <td>10.734571</td>\n",
       "      <td>17.713216</td>\n",
       "      <td>33.13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    model       nll     brier        ece        mce  classification error\n",
       "0      17  1.049376  0.467641  11.006592  17.914299                 32.97\n",
       "1      18  1.120071  0.492364  12.722073  22.916308                 34.26\n",
       "2      19  1.081817  0.472275  10.947023  19.122961                 33.28\n",
       "3      20  1.035384  0.459745  10.809647  19.084227                 32.63\n",
       "4      21  1.129620  0.488162  13.275932  26.464154                 34.49\n",
       "5      22  1.046511  0.461049  12.011800  19.272041                 32.47\n",
       "6      23  1.049194  0.463736  11.148562  19.476030                 32.28\n",
       "7      24  1.030083  0.459401  10.782473  18.340129                 32.60\n",
       "8      25  1.054359  0.465495  12.315030  20.277822                 32.66\n",
       "9      26  1.080598  0.472686  11.697054  18.425075                 33.13\n",
       "10     27  1.029928  0.455730  11.239438  19.754712                 31.91\n",
       "11     28  1.055803  0.464551  11.917404  19.966303                 32.62\n",
       "12     29  1.034507  0.453163  11.403457  23.194063                 31.71\n",
       "13     30  1.059232  0.463348  11.684049  80.103070                 32.92\n",
       "14     31  1.102552  0.484832  12.436964  22.873992                 34.30\n",
       "15     32  1.056176  0.465262  11.579560  19.582395                 32.93\n",
       "16     33  1.100861  0.478181  13.072742  24.500363                 33.64\n",
       "17     34  1.039079  0.464342  11.304843  20.091574                 33.18\n",
       "18     35  1.077137  0.480872  12.100875  20.664407                 33.92\n",
       "19     36  1.041124  0.463309  11.243741  20.097279                 32.56\n",
       "20     37  1.072076  0.476303  12.533180  20.652961                 33.32\n",
       "21     38  1.064487  0.472570  11.019156  20.153915                 33.18\n",
       "22     39  1.045349  0.464439  11.604372  18.856452                 32.85\n",
       "23     40  1.074239  0.468064  12.351936  21.197368                 32.63\n",
       "24     41  1.047256  0.468077  10.734571  17.713216                 33.13"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASELINE RESULTS (TEST SET)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "NLL\n",
      "1.063 ± 0.03\n",
      "--------------------\n",
      "BRIER\n",
      "0.469 ± 0.01\n",
      "--------------------\n",
      "ECE\n",
      "11.718 ± 0.72\n",
      "--------------------\n",
      "MCE\n",
      "22.828 ± 11.87\n",
      "--------------------\n",
      "CLASSIFICATION ERROR\n",
      "33.023 ± 0.68\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "print('Baseline Results (test set)'.upper())\n",
    "print('-'*100)\n",
    "metrics = ['nll', 'brier', 'ece', 'mce', 'classification error']\n",
    "for metric in metrics:\n",
    "    print('{0}'.format(metric.upper()))\n",
    "    print('{0:.3f} \\u00B1 {1:.2f}'.format(np.mean(df_baselines[metric]), np.std(df_baselines[metric])))\n",
    "    print('-'*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Increase number of models and do bootstrapping for DE to provide std..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bootstrap = 100\n",
    "m_deep_ensemble = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = []\n",
    "for i in range(n_bootstrap):\n",
    "    model_indices = np.random.choice(np.shape(all_test_preds)[0], size=m_deep_ensemble, replace=False)\n",
    "    y_pred = np.mean(np.stack(all_test_preds[model_indices], axis=0), axis=0)\n",
    "    nll = log_loss(y_test, y_pred)\n",
    "    error = classification_error(y_test, y_pred)\n",
    "    br = brier_multi(y_test, y_pred)\n",
    "    calib_erros = CalibrationErrors(y_test, y_pred, bin_size=1 / 20., min_samples=0)\n",
    "    _, _, ece, mce, _ = calib_erros.calculate_calibration_errors()\n",
    "    d.append(OrderedDict({\"model\":model_id, \"nll\": nll, \"brier\": br, \"ece\": ece, \"mce\":mce, \"classification error\": error}))\n",
    "df_deep_ensembles = pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>nll</th>\n",
       "      <th>brier</th>\n",
       "      <th>ece</th>\n",
       "      <th>mce</th>\n",
       "      <th>classification error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>0.703052</td>\n",
       "      <td>0.334290</td>\n",
       "      <td>8.605198</td>\n",
       "      <td>19.944169</td>\n",
       "      <td>22.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>0.707648</td>\n",
       "      <td>0.335802</td>\n",
       "      <td>8.541378</td>\n",
       "      <td>19.289700</td>\n",
       "      <td>23.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>0.706949</td>\n",
       "      <td>0.334073</td>\n",
       "      <td>8.845972</td>\n",
       "      <td>15.891629</td>\n",
       "      <td>22.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>41</td>\n",
       "      <td>0.709211</td>\n",
       "      <td>0.335223</td>\n",
       "      <td>9.293659</td>\n",
       "      <td>17.280933</td>\n",
       "      <td>22.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>41</td>\n",
       "      <td>0.706328</td>\n",
       "      <td>0.334914</td>\n",
       "      <td>8.730393</td>\n",
       "      <td>15.190134</td>\n",
       "      <td>22.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>41</td>\n",
       "      <td>0.705287</td>\n",
       "      <td>0.333100</td>\n",
       "      <td>8.951728</td>\n",
       "      <td>25.695055</td>\n",
       "      <td>22.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>41</td>\n",
       "      <td>0.704011</td>\n",
       "      <td>0.333073</td>\n",
       "      <td>8.804734</td>\n",
       "      <td>19.282881</td>\n",
       "      <td>22.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>41</td>\n",
       "      <td>0.711918</td>\n",
       "      <td>0.336928</td>\n",
       "      <td>8.960777</td>\n",
       "      <td>14.611319</td>\n",
       "      <td>22.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>41</td>\n",
       "      <td>0.706945</td>\n",
       "      <td>0.334302</td>\n",
       "      <td>8.519659</td>\n",
       "      <td>15.706914</td>\n",
       "      <td>23.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>41</td>\n",
       "      <td>0.708254</td>\n",
       "      <td>0.336609</td>\n",
       "      <td>8.618250</td>\n",
       "      <td>15.805352</td>\n",
       "      <td>22.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    model       nll     brier       ece        mce  classification error\n",
       "0      41  0.703052  0.334290  8.605198  19.944169                 22.82\n",
       "1      41  0.707648  0.335802  8.541378  19.289700                 23.04\n",
       "2      41  0.706949  0.334073  8.845972  15.891629                 22.94\n",
       "3      41  0.709211  0.335223  9.293659  17.280933                 22.54\n",
       "4      41  0.706328  0.334914  8.730393  15.190134                 22.87\n",
       "..    ...       ...       ...       ...        ...                   ...\n",
       "95     41  0.705287  0.333100  8.951728  25.695055                 22.66\n",
       "96     41  0.704011  0.333073  8.804734  19.282881                 22.72\n",
       "97     41  0.711918  0.336928  8.960777  14.611319                 22.93\n",
       "98     41  0.706945  0.334302  8.519659  15.706914                 23.08\n",
       "99     41  0.708254  0.336609  8.618250  15.805352                 22.99\n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_deep_ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEEP ENSEMBLE RESULTS (TEST SET)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "NLL\n",
      "0.709 ± 0.00\n",
      "--------------------\n",
      "BRIER\n",
      "0.336 ± 0.00\n",
      "--------------------\n",
      "ECE\n",
      "8.862 ± 0.25\n",
      "--------------------\n",
      "MCE\n",
      "18.757 ± 10.50\n",
      "--------------------\n",
      "CLASSIFICATION ERROR\n",
      "22.888 ± 0.19\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "print('Deep Ensemble Results (test set)'.upper())\n",
    "print('-'*100)\n",
    "metrics = ['nll', 'brier', 'ece', 'mce', 'classification error']\n",
    "for metric in metrics:\n",
    "    print('{0}'.format(metric.upper()))\n",
    "    print('{0:.3f} \\u00B1 {1:.2f}'.format(np.mean(df_deep_ensembles[metric]), np.std(df_deep_ensembles[metric])))\n",
    "    print('-'*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temperature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "def softmax_t(y_logit, t):\n",
    "    return np.exp(y_logit/t)/np.sum(np.exp(y_logit/t), axis=-1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "def ll_t(t):\n",
    "    y_val_pred = model.predict(x_val)\n",
    "    y_pred_base_logit = np.log(y_val_pred)\n",
    "    y_pred_temp = softmax_t(y_pred_base_logit, t)\n",
    "    ll = log_loss(y_val, y_pred_temp)\n",
    "    return ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.52187162]\n",
      " [1.60517768]\n",
      " [1.54157578]\n",
      " [1.51044265]\n",
      " [1.60736797]\n",
      " [1.56715476]\n",
      " [1.53729228]\n",
      " [1.49686581]\n",
      " [1.61902562]\n",
      " [1.58160925]\n",
      " [1.56568306]\n",
      " [1.54354629]\n",
      " [1.54358672]\n",
      " [1.55429627]\n",
      " [1.59015824]\n",
      " [1.5591671 ]\n",
      " [1.65176978]\n",
      " [1.48680123]\n",
      " [1.53217427]\n",
      " [1.54044825]\n",
      " [1.60382846]\n",
      " [1.5598986 ]\n",
      " [1.57179334]\n",
      " [1.60726693]\n",
      " [1.49423654]]\n"
     ]
    }
   ],
   "source": [
    "optimal_temps = []\n",
    "optimal_temps_path = os.path.join(output_baseline_folder, 'optimal_temps.csv')\n",
    "if not os.path.isfile(optimal_temps_path):\n",
    "# if True:\n",
    "    for index, model_id in enumerate(range(17, 17+ n_models)):\n",
    "        print('finding optimal sigma for model {}...'.format(model_id))\n",
    "        model_folder = os.path.join(output_baseline_folder, str(model_id))\n",
    "        checkpoint_path = glob.glob( model_folder + '/weights.' + str(epochs).zfill(2) + '*.hdf5')[0]\n",
    "        model = cnn()\n",
    "        print('loading weights...')\n",
    "        model.load_weights(checkpoint_path)\n",
    "        print('running optimization...')\n",
    "        xopt = minimize(ll_t, 1, method='bfgs', options={'disp': 1})\n",
    "        optimal_temp = xopt['x'][0]\n",
    "        print('-'*100)\n",
    "        print(model_id, optimal_temp)\n",
    "        optimal_temps.append(optimal_temp)\n",
    "    pd.DataFrame(optimal_temps, columns=['optimal temp']).to_csv(optimal_temps_path, index=False)\n",
    "else:\n",
    "    optimal_temps = pd.read_csv(optimal_temps_path).values\n",
    "print(optimal_temps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, model_id in enumerate(range(17, 17 + n_models)):\n",
    "    model_folder = os.path.join(output_baseline_folder, str(model_id))\n",
    "    output_ts_folder = os.path.join(model_folder, 'temp_scaled')\n",
    "    if not os.path.isdir(output_ts_folder):\n",
    "        os.mkdir(output_ts_folder)\n",
    "    y_pred = np.load(os.path.join(model_folder, 'test_pred.npy'))\n",
    "    optimal_temp = optimal_temps[index]\n",
    "    #\n",
    "    y_pred_logit = np.log(y_pred)\n",
    "    y_pred_ts = softmax_t(y_pred_logit, t=optimal_temp)\n",
    "    #\n",
    "    np.save(os.path.join(output_ts_folder, 'test_pred_ts.npy'), y_pred_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = list()\n",
    "for index, model_id in enumerate(range(17, 17 + n_models)):\n",
    "    model_ppe_folder= os.path.join(output_baseline_folder, str(model_id),  'temp_scaled')\n",
    "    y_pred = np.load(os.path.join(model_ppe_folder, 'test_pred_ts.npy'))\n",
    "    nll = log_loss(y_test, y_pred)\n",
    "    error = classification_error(y_test, y_pred)\n",
    "    br = brier_multi(y_test, y_pred)\n",
    "    calib_erros = CalibrationErrors(y_test, y_pred, bin_size=1 / 20., min_samples=0)\n",
    "    _, _, ece, mce, _ = calib_erros.calculate_calibration_errors()\n",
    "    d.append(OrderedDict({\"model\":model_id, \"nll\": nll, \"brier\": br, \"ece\": ece, \"mce\":mce, \"classification error\": error}))\n",
    "df_ts= pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>nll</th>\n",
       "      <th>brier</th>\n",
       "      <th>ece</th>\n",
       "      <th>mce</th>\n",
       "      <th>classification error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0.958457</td>\n",
       "      <td>0.447446</td>\n",
       "      <td>0.990291</td>\n",
       "      <td>30.908940</td>\n",
       "      <td>32.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0.992839</td>\n",
       "      <td>0.464118</td>\n",
       "      <td>1.559196</td>\n",
       "      <td>4.011182</td>\n",
       "      <td>34.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>0.980082</td>\n",
       "      <td>0.454177</td>\n",
       "      <td>1.329692</td>\n",
       "      <td>6.075970</td>\n",
       "      <td>33.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>0.947025</td>\n",
       "      <td>0.440444</td>\n",
       "      <td>1.400703</td>\n",
       "      <td>19.024498</td>\n",
       "      <td>32.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>0.998796</td>\n",
       "      <td>0.461397</td>\n",
       "      <td>1.930012</td>\n",
       "      <td>12.155861</td>\n",
       "      <td>34.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>0.934973</td>\n",
       "      <td>0.438440</td>\n",
       "      <td>1.647617</td>\n",
       "      <td>6.757016</td>\n",
       "      <td>32.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>0.945599</td>\n",
       "      <td>0.442241</td>\n",
       "      <td>1.279003</td>\n",
       "      <td>4.768501</td>\n",
       "      <td>32.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>24</td>\n",
       "      <td>0.942112</td>\n",
       "      <td>0.440862</td>\n",
       "      <td>1.024590</td>\n",
       "      <td>4.919921</td>\n",
       "      <td>32.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>25</td>\n",
       "      <td>0.937399</td>\n",
       "      <td>0.441105</td>\n",
       "      <td>0.879876</td>\n",
       "      <td>6.105325</td>\n",
       "      <td>32.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>26</td>\n",
       "      <td>0.967025</td>\n",
       "      <td>0.450670</td>\n",
       "      <td>1.112539</td>\n",
       "      <td>14.335243</td>\n",
       "      <td>33.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>27</td>\n",
       "      <td>0.926706</td>\n",
       "      <td>0.434347</td>\n",
       "      <td>1.413073</td>\n",
       "      <td>5.187482</td>\n",
       "      <td>31.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>28</td>\n",
       "      <td>0.946639</td>\n",
       "      <td>0.442292</td>\n",
       "      <td>1.261804</td>\n",
       "      <td>8.951931</td>\n",
       "      <td>32.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>29</td>\n",
       "      <td>0.925973</td>\n",
       "      <td>0.431991</td>\n",
       "      <td>1.508459</td>\n",
       "      <td>18.473283</td>\n",
       "      <td>31.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>30</td>\n",
       "      <td>0.953622</td>\n",
       "      <td>0.441949</td>\n",
       "      <td>1.513690</td>\n",
       "      <td>3.541463</td>\n",
       "      <td>32.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>31</td>\n",
       "      <td>0.987907</td>\n",
       "      <td>0.459672</td>\n",
       "      <td>1.629416</td>\n",
       "      <td>18.781697</td>\n",
       "      <td>34.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>32</td>\n",
       "      <td>0.951860</td>\n",
       "      <td>0.444522</td>\n",
       "      <td>1.108392</td>\n",
       "      <td>4.139278</td>\n",
       "      <td>32.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>33</td>\n",
       "      <td>0.967029</td>\n",
       "      <td>0.449702</td>\n",
       "      <td>1.416862</td>\n",
       "      <td>11.628896</td>\n",
       "      <td>33.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>34</td>\n",
       "      <td>0.948110</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>1.498599</td>\n",
       "      <td>5.318780</td>\n",
       "      <td>33.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>35</td>\n",
       "      <td>0.969929</td>\n",
       "      <td>0.456899</td>\n",
       "      <td>1.442398</td>\n",
       "      <td>4.370966</td>\n",
       "      <td>33.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>36</td>\n",
       "      <td>0.944324</td>\n",
       "      <td>0.442137</td>\n",
       "      <td>1.055835</td>\n",
       "      <td>25.033170</td>\n",
       "      <td>32.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>37</td>\n",
       "      <td>0.956730</td>\n",
       "      <td>0.450185</td>\n",
       "      <td>1.623082</td>\n",
       "      <td>18.545955</td>\n",
       "      <td>33.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>38</td>\n",
       "      <td>0.967404</td>\n",
       "      <td>0.452070</td>\n",
       "      <td>1.012037</td>\n",
       "      <td>18.776168</td>\n",
       "      <td>33.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>39</td>\n",
       "      <td>0.943872</td>\n",
       "      <td>0.442620</td>\n",
       "      <td>1.211197</td>\n",
       "      <td>6.405414</td>\n",
       "      <td>32.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>40</td>\n",
       "      <td>0.948013</td>\n",
       "      <td>0.442655</td>\n",
       "      <td>1.035598</td>\n",
       "      <td>4.594197</td>\n",
       "      <td>32.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>41</td>\n",
       "      <td>0.960550</td>\n",
       "      <td>0.449437</td>\n",
       "      <td>1.065465</td>\n",
       "      <td>5.899719</td>\n",
       "      <td>33.13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    model       nll     brier       ece        mce  classification error\n",
       "0      17  0.958457  0.447446  0.990291  30.908940                 32.97\n",
       "1      18  0.992839  0.464118  1.559196   4.011182                 34.26\n",
       "2      19  0.980082  0.454177  1.329692   6.075970                 33.28\n",
       "3      20  0.947025  0.440444  1.400703  19.024498                 32.63\n",
       "4      21  0.998796  0.461397  1.930012  12.155861                 34.49\n",
       "5      22  0.934973  0.438440  1.647617   6.757016                 32.47\n",
       "6      23  0.945599  0.442241  1.279003   4.768501                 32.28\n",
       "7      24  0.942112  0.440862  1.024590   4.919921                 32.60\n",
       "8      25  0.937399  0.441105  0.879876   6.105325                 32.66\n",
       "9      26  0.967025  0.450670  1.112539  14.335243                 33.13\n",
       "10     27  0.926706  0.434347  1.413073   5.187482                 31.91\n",
       "11     28  0.946639  0.442292  1.261804   8.951931                 32.62\n",
       "12     29  0.925973  0.431991  1.508459  18.473283                 31.71\n",
       "13     30  0.953622  0.441949  1.513690   3.541463                 32.92\n",
       "14     31  0.987907  0.459672  1.629416  18.781697                 34.30\n",
       "15     32  0.951860  0.444522  1.108392   4.139278                 32.93\n",
       "16     33  0.967029  0.449702  1.416862  11.628896                 33.64\n",
       "17     34  0.948110  0.444444  1.498599   5.318780                 33.18\n",
       "18     35  0.969929  0.456899  1.442398   4.370966                 33.92\n",
       "19     36  0.944324  0.442137  1.055835  25.033170                 32.56\n",
       "20     37  0.956730  0.450185  1.623082  18.545955                 33.32\n",
       "21     38  0.967404  0.452070  1.012037  18.776168                 33.18\n",
       "22     39  0.943872  0.442620  1.211197   6.405414                 32.85\n",
       "23     40  0.948013  0.442655  1.035598   4.594197                 32.63\n",
       "24     41  0.960550  0.449437  1.065465   5.899719                 33.13"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEMPERATURE SCALING RESULTS (TEST SET)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "NLL\n",
      "0.956 ± 0.02\n",
      "--------------------\n",
      "BRIER\n",
      "0.447 ± 0.01\n",
      "--------------------\n",
      "ECE\n",
      "1.318 ± 0.26\n",
      "--------------------\n",
      "MCE\n",
      "10.748 ± 7.48\n",
      "--------------------\n",
      "CLASSIFICATION ERROR\n",
      "33.023 ± 0.68\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "print('Temperature Scaling Results (test set)'.upper())\n",
    "print('-'*100)\n",
    "metrics = ['nll', 'brier', 'ece', 'mce', 'classification error']\n",
    "for metric in metrics:\n",
    "    print('{0}'.format(metric.upper()))\n",
    "    print('{0:.3f} \\u00B1 {1:.2f}'.format(np.mean(df_ts[metric]), np.std(df_ts[metric])))\n",
    "    print('-'*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPE:\n",
    "    def __init__(self, model, n_ensemble):\n",
    "        self.model = model\n",
    "        self.n_ensemble = n_ensemble\n",
    "        self.original_weights = model.get_weights()\n",
    "        \n",
    "    def perturb(self, sigma):\n",
    "        self.model.set_weights(self.original_weights)\n",
    "        weights = self.model.get_weights()\n",
    "        for index2, w in enumerate(weights):\n",
    "            shape = w.shape\n",
    "            if len(shape) >= 2  and shape[-1] > 32:\n",
    "                if len(shape) == 4:\n",
    "                    noise = np.random.normal(0, sigma, (w.shape[0], w.shape[1], w.shape[2], w.shape[3]))\n",
    "                else:\n",
    "                    noise = np.random.normal(0, sigma, (w.shape[0], w.shape[1]))\n",
    "                weights[index2] = weights[index2] + noise\n",
    "        self.model.set_weights(weights)\n",
    "    \n",
    "    def ensemble_pred(self, x, sigma):\n",
    "        y_pred = self.model.predict(x)\n",
    "        y_pred_ensemble = np.zeros((self.n_ensemble, *y_pred.shape))\n",
    "        for seed_index, seed in enumerate(range(17, 17 + self.n_ensemble)):\n",
    "            np.random.seed(seed)\n",
    "            self.perturb(sigma)\n",
    "            y_pred = self.model.predict(x)\n",
    "            y_pred_ensemble[seed_index] = y_pred\n",
    "        self.model.set_weights(self.original_weights)\n",
    "        return np.mean(y_pred_ensemble, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def golden_search(model, sigma_lower=0, sigma_upper=0.1, n_ppe_ensemble=10, n_iterations=10):\n",
    "    sigmas_log = list()\n",
    "    nlls_log = list()\n",
    "    nlls = dict()\n",
    "    PHI = (np.sqrt(5) - 1) / 2\n",
    "    for iteration in range(n_iterations):\n",
    "        print('iteration {} {} {} '.format(iteration, sigma_lower, sigma_upper) + '-'* 10)\n",
    "        sigma_1 = sigma_lower + (sigma_upper - sigma_lower) * PHI\n",
    "        sigma_2 = sigma_lower + (sigma_1 - sigma_lower) * PHI\n",
    "        sigmas_log.append(OrderedDict({\"iteration\": iteration, \"sigma_l\": sigma_lower,\n",
    "                                       \"sigma_2\": sigma_2, \"sigma_1\": sigma_1,\n",
    "                                       \"sigma_h\": sigma_upper}))\n",
    "        for sigma in [sigma_1, sigma_2]:\n",
    "            if sigma not in nlls.keys():\n",
    "                ppe = PPE(model, n_ensemble=n_ppe_ensemble)\n",
    "                y_val_ppe = ppe.ensemble_pred(x_val, sigma)\n",
    "                nll = log_loss(y_val, y_val_ppe)\n",
    "                nlls[sigma] = nll\n",
    "                print(sigma, nll)\n",
    "                nlls_log.append(OrderedDict({\"iteration\": iteration, \"sigma\": sigma, \"nll\": nll}))\n",
    "            else:\n",
    "                print('will skip for {0:6f}'.format(sigma))\n",
    "        if nlls[sigma_1] < nlls[sigma_2]:\n",
    "            sigma_lower = sigma_2\n",
    "            print('updated lower side...')\n",
    "        else:\n",
    "            sigma_upper = sigma_1\n",
    "            print('updated upper side...')\n",
    "    sigma_optimal = (sigmas_log[-1]['sigma_l'] + sigmas_log[-1]['sigma_h'])/2\n",
    "    return sigma_optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02294902 0.02294902 0.02294902 0.01950483 0.02720626 0.02163346\n",
      " 0.02294902 0.01818927 0.02213595 0.02294902 0.02082039 0.02163346\n",
      " 0.02294902 0.02163346 0.02376208 0.02376208 0.02426458 0.02163346\n",
      " 0.02082039 0.02213595 0.02294902 0.02376208 0.02213595 0.02294902\n",
      " 0.02213595]\n"
     ]
    }
   ],
   "source": [
    "optimal_sigmas = []\n",
    "optimal_sigmas_path = os.path.join(output_baseline_folder, 'golden_optimization_simgas.csv')\n",
    "if not os.path.isfile(optimal_sigmas_path):\n",
    "# if True:\n",
    "    for index, model_id in enumerate(range(17, 17 + n_models)):\n",
    "        print('finding optimal sigma for model {}...'.format(model_id))\n",
    "        model_folder = os.path.join(output_baseline_folder, str(model_id))\n",
    "        checkpoint_path = glob.glob( model_folder + '/weights.' + str(epochs).zfill(2) + '*.hdf5')[0]\n",
    "        model = cnn()\n",
    "        print('loading weights...')\n",
    "        model.load_weights(checkpoint_path)\n",
    "        print('running optimization...')\n",
    "        optimal_sigma = golden_search(model)\n",
    "        optimal_sigmas.append(optimal_sigma)\n",
    "    df = pd.DataFrame(optimal_sigmas, columns=['optimal sigma'])\n",
    "    df.to_csv(optimal_sigmas_path, index=False)\n",
    "else:\n",
    "    df = pd.read_csv(optimal_sigmas_path)\n",
    "optimal_sigmas = df['optimal sigma'].values\n",
    "print(optimal_sigmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02294902 0.02294902 0.02294902 0.01950483 0.02720626 0.02163346\n",
      " 0.02294902 0.01818927 0.02213595 0.02294902 0.02082039 0.02163346\n",
      " 0.02294902 0.02163346 0.02376208 0.02376208 0.02426458 0.02163346\n",
      " 0.02082039 0.02213595 0.02294902 0.02376208 0.02213595 0.02294902\n",
      " 0.02213595]\n"
     ]
    }
   ],
   "source": [
    "print(optimal_sigmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ppe_ensemble = 25\n",
    "for index, model_id in enumerate(range(17, 17+n_models)):\n",
    "    model_folder = os.path.join(output_baseline_folder, str(model_id))\n",
    "    output_ppe_folder = os.path.join(model_folder, 'ppe')\n",
    "    if not os.path.isdir(output_ppe_folder):\n",
    "        os.mkdir(output_ppe_folder)\n",
    "    test_pred_path = os.path.join(output_ppe_folder, 'test_pred_ppe.npy')\n",
    "    if not os.path.isfile(test_pred_path):\n",
    "    # if True:\n",
    "        checkpoint_path = glob.glob(model_folder + '/weights.' + str(epochs).zfill(2) + '*.hdf5')[0]\n",
    "        model = cnn()\n",
    "        print('loading weights for model {}...'.format(model_id))\n",
    "        model.load_weights(checkpoint_path)\n",
    "        optimal_sigma = optimal_sigmas[index]\n",
    "        ppe = PPE(model, n_ensemble=n_ppe_ensemble)\n",
    "        y_pred = ppe.ensemble_pred(x_test, optimal_sigma)\n",
    "        np.save(test_pred_path, y_pred)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = list()\n",
    "for index, model_id in enumerate(range(17, 17 + n_models)):\n",
    "    model_ppe_folder= os.path.join(output_baseline_folder, str(model_id),  'ppe')\n",
    "    y_pred = np.load(os.path.join(model_ppe_folder, 'test_pred_ppe.npy'))\n",
    "    nll = log_loss(y_test, y_pred)\n",
    "    error = classification_error(y_test, y_pred)\n",
    "    br = brier_multi(y_test, y_pred)\n",
    "    calib_erros = CalibrationErrors(y_test, y_pred, bin_size=1 / 20., min_samples=0)\n",
    "    _, _, ece, mce, _ = calib_erros.calculate_calibration_errors()\n",
    "    d.append(OrderedDict({\"model\":model_id, \"nll\": nll, \"brier\": br, \"ece\": ece, \"mce\":mce, \"classification error\": error}))\n",
    "df_ppe = pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>nll</th>\n",
       "      <th>brier</th>\n",
       "      <th>ece</th>\n",
       "      <th>mce</th>\n",
       "      <th>classification error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0.974168</td>\n",
       "      <td>0.448255</td>\n",
       "      <td>3.563987</td>\n",
       "      <td>81.138025</td>\n",
       "      <td>32.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>1.028044</td>\n",
       "      <td>0.469613</td>\n",
       "      <td>5.869813</td>\n",
       "      <td>18.479801</td>\n",
       "      <td>34.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>1.001438</td>\n",
       "      <td>0.453893</td>\n",
       "      <td>3.249139</td>\n",
       "      <td>21.162407</td>\n",
       "      <td>33.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>0.978203</td>\n",
       "      <td>0.446278</td>\n",
       "      <td>5.432041</td>\n",
       "      <td>19.648206</td>\n",
       "      <td>32.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>1.019923</td>\n",
       "      <td>0.464015</td>\n",
       "      <td>3.197253</td>\n",
       "      <td>6.467382</td>\n",
       "      <td>34.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>0.963040</td>\n",
       "      <td>0.442762</td>\n",
       "      <td>4.933659</td>\n",
       "      <td>30.632605</td>\n",
       "      <td>32.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>0.979155</td>\n",
       "      <td>0.449929</td>\n",
       "      <td>4.559371</td>\n",
       "      <td>14.577703</td>\n",
       "      <td>32.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>24</td>\n",
       "      <td>0.977900</td>\n",
       "      <td>0.446258</td>\n",
       "      <td>6.019534</td>\n",
       "      <td>16.974282</td>\n",
       "      <td>32.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>25</td>\n",
       "      <td>0.967593</td>\n",
       "      <td>0.444355</td>\n",
       "      <td>4.820005</td>\n",
       "      <td>8.089263</td>\n",
       "      <td>32.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>26</td>\n",
       "      <td>0.996202</td>\n",
       "      <td>0.454179</td>\n",
       "      <td>4.246012</td>\n",
       "      <td>19.413000</td>\n",
       "      <td>33.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>27</td>\n",
       "      <td>0.956050</td>\n",
       "      <td>0.437510</td>\n",
       "      <td>5.078880</td>\n",
       "      <td>19.129462</td>\n",
       "      <td>31.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>28</td>\n",
       "      <td>0.972050</td>\n",
       "      <td>0.444054</td>\n",
       "      <td>5.037469</td>\n",
       "      <td>19.240016</td>\n",
       "      <td>32.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>29</td>\n",
       "      <td>0.940015</td>\n",
       "      <td>0.429794</td>\n",
       "      <td>3.579875</td>\n",
       "      <td>80.220877</td>\n",
       "      <td>31.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>30</td>\n",
       "      <td>0.975148</td>\n",
       "      <td>0.443088</td>\n",
       "      <td>4.753868</td>\n",
       "      <td>22.899275</td>\n",
       "      <td>32.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>31</td>\n",
       "      <td>1.017367</td>\n",
       "      <td>0.462033</td>\n",
       "      <td>4.691815</td>\n",
       "      <td>17.029499</td>\n",
       "      <td>34.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>32</td>\n",
       "      <td>0.967964</td>\n",
       "      <td>0.445298</td>\n",
       "      <td>3.181704</td>\n",
       "      <td>47.567858</td>\n",
       "      <td>32.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>33</td>\n",
       "      <td>1.007644</td>\n",
       "      <td>0.456060</td>\n",
       "      <td>5.364299</td>\n",
       "      <td>18.728922</td>\n",
       "      <td>33.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>34</td>\n",
       "      <td>0.969408</td>\n",
       "      <td>0.446942</td>\n",
       "      <td>4.943681</td>\n",
       "      <td>18.611190</td>\n",
       "      <td>33.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>35</td>\n",
       "      <td>1.001959</td>\n",
       "      <td>0.460883</td>\n",
       "      <td>5.467997</td>\n",
       "      <td>17.614136</td>\n",
       "      <td>33.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>36</td>\n",
       "      <td>0.965984</td>\n",
       "      <td>0.444052</td>\n",
       "      <td>4.628952</td>\n",
       "      <td>19.079488</td>\n",
       "      <td>32.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>37</td>\n",
       "      <td>0.983468</td>\n",
       "      <td>0.452418</td>\n",
       "      <td>4.638436</td>\n",
       "      <td>18.993719</td>\n",
       "      <td>33.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>38</td>\n",
       "      <td>0.993707</td>\n",
       "      <td>0.454471</td>\n",
       "      <td>3.649302</td>\n",
       "      <td>19.197333</td>\n",
       "      <td>33.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>39</td>\n",
       "      <td>0.969301</td>\n",
       "      <td>0.445613</td>\n",
       "      <td>4.524315</td>\n",
       "      <td>18.768113</td>\n",
       "      <td>32.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>40</td>\n",
       "      <td>0.982685</td>\n",
       "      <td>0.448373</td>\n",
       "      <td>5.646752</td>\n",
       "      <td>19.432410</td>\n",
       "      <td>33.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>41</td>\n",
       "      <td>0.972274</td>\n",
       "      <td>0.448403</td>\n",
       "      <td>3.909245</td>\n",
       "      <td>7.084717</td>\n",
       "      <td>33.04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    model       nll     brier       ece        mce  classification error\n",
       "0      17  0.974168  0.448255  3.563987  81.138025                 32.80\n",
       "1      18  1.028044  0.469613  5.869813  18.479801                 34.73\n",
       "2      19  1.001438  0.453893  3.249139  21.162407                 33.03\n",
       "3      20  0.978203  0.446278  5.432041  19.648206                 32.88\n",
       "4      21  1.019923  0.464015  3.197253   6.467382                 34.01\n",
       "5      22  0.963040  0.442762  4.933659  30.632605                 32.34\n",
       "6      23  0.979155  0.449929  4.559371  14.577703                 32.86\n",
       "7      24  0.977900  0.446258  6.019534  16.974282                 32.74\n",
       "8      25  0.967593  0.444355  4.820005   8.089263                 32.36\n",
       "9      26  0.996202  0.454179  4.246012  19.413000                 33.13\n",
       "10     27  0.956050  0.437510  5.078880  19.129462                 31.83\n",
       "11     28  0.972050  0.444054  5.037469  19.240016                 32.14\n",
       "12     29  0.940015  0.429794  3.579875  80.220877                 31.19\n",
       "13     30  0.975148  0.443088  4.753868  22.899275                 32.59\n",
       "14     31  1.017367  0.462033  4.691815  17.029499                 34.19\n",
       "15     32  0.967964  0.445298  3.181704  47.567858                 32.44\n",
       "16     33  1.007644  0.456060  5.364299  18.728922                 33.83\n",
       "17     34  0.969408  0.446942  4.943681  18.611190                 33.20\n",
       "18     35  1.001959  0.460883  5.467997  17.614136                 33.68\n",
       "19     36  0.965984  0.444052  4.628952  19.079488                 32.55\n",
       "20     37  0.983468  0.452418  4.638436  18.993719                 33.01\n",
       "21     38  0.993707  0.454471  3.649302  19.197333                 33.33\n",
       "22     39  0.969301  0.445613  4.524315  18.768113                 32.69\n",
       "23     40  0.982685  0.448373  5.646752  19.432410                 33.14\n",
       "24     41  0.972274  0.448403  3.909245   7.084717                 33.04"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ppe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPE RESULTS (TEST SET)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "NLL\n",
      "0.982 ± 0.02\n",
      "--------------------\n",
      "BRIER\n",
      "0.450 ± 0.01\n",
      "--------------------\n",
      "ECE\n",
      "4.599 ± 0.82\n",
      "--------------------\n",
      "MCE\n",
      "24.007 ± 18.33\n",
      "--------------------\n",
      "CLASSIFICATION ERROR\n",
      "32.949 ± 0.74\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "print('PPE Results (test set)'.upper())\n",
    "print('-'*100)\n",
    "metrics = ['nll', 'brier', 'ece', 'mce', 'classification error']\n",
    "for metric in metrics:\n",
    "    print('{0}'.format(metric.upper()))\n",
    "    print('{0:.3f} \\u00B1 {1:.2f}'.format(np.mean(df_ppe[metric]), np.std(df_ppe[metric])))\n",
    "    print('-'*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MC Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_mcdo(num_classes=10, dropout=True, level=0.5):\n",
    "    img_rows, img_cols = 32, 32\n",
    "    input_shape = (img_rows, img_cols, 3)\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(16, (3, 3), padding='same', input_shape=input_shape, activation='relu'))\n",
    "    model.add(BatchNormalization(momentum=.9))\n",
    "    \n",
    "    model.add(Conv2D(16, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization(momentum=.9))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization(momentum=.9))\n",
    "    if dropout:\n",
    "        model.add(Lambda(lambda x: K.dropout(x, level=level)))\n",
    "    \n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization(momentum=.9))\n",
    "    if dropout:\n",
    "        model.add(Lambda(lambda x: K.dropout(x, level=level)))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(BatchNormalization(momentum=.9))\n",
    "    if dropout:\n",
    "        model.add(Lambda(lambda x: K.dropout(x, level=level)))\n",
    "    \n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 5e-4\n",
    "#\n",
    "output_baseline_dropout_folder = os.path.join(output_folder, 'baselines_dropout')\n",
    "if not os.path.isdir(output_baseline_dropout_folder):\n",
    "    os.makedirs(output_baseline_dropout_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(n_models):\n",
    "    seed = i+17\n",
    "    # change the random state\n",
    "    np.random.seed(seed)\n",
    "    # creating output folder for each of the baseline models\n",
    "    output_model_folder = os.path.join(output_baseline_dropout_folder, str(seed))\n",
    "    if not os.path.isdir(output_model_folder):\n",
    "        os.mkdir(output_model_folder)\n",
    "        # callbacks for training\n",
    "        callbacks = list()\n",
    "        log = CSVLogger(os.path.join(output_model_folder, 'log.csv'))\n",
    "        callbacks.append(log)\n",
    "        checkpoint_path = os.path.join(output_model_folder, 'weights.{epoch:02d}.hdf5')\n",
    "        checkpoint = ModelCheckpoint(checkpoint_path, period=1, save_best_only=False, verbose=True)\n",
    "        callbacks.append(checkpoint)\n",
    "        # initialize and compile the model\n",
    "        model = cnn_mcdo()\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=learning_rate), metrics=['accuracy'])\n",
    "        # train\n",
    "        model.fit(x_train, y_train, batch_size=128, epochs=15, verbose=1, callbacks=callbacks, validation_data=(x_val, y_val), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(17, 17+n_models):\n",
    "    log = pd.read_csv(os.path.join(output_baseline_dropout_folder, str(i), 'log.csv'))\n",
    "    plt.figure(figsize=(8,2),dpi=100)\n",
    "    plt.subplots_adjust(wspace=0.5)\n",
    "    ax1 = plt.subplot(121)\n",
    "    ax2 = plt.subplot(122)\n",
    "    for metric_type in ['', 'val_']:\n",
    "        for key in log.keys():\n",
    "            if 'loss' in key and 'val' not in key:\n",
    "                p = ax1.plot(log[metric_type+ key], label=metric_type + key+str(i), linewidth=1)\n",
    "                c = p[-1].get_color()\n",
    "                ax1.plot(np.argmin(log[metric_type+key]), np.amin(log[metric_type+key]), \n",
    "                         '*', markersize=10, alpha=0.5, color = c)\n",
    "                ax1.set_title('min val NLL: {0:.3f}'.format(np.amin(log['val_' + key])))\n",
    "                ax1.set_ylabel('NLL')\n",
    "                ax1.set_xlabel('epochs')\n",
    "            if 'acc' in key and 'val' not in key:\n",
    "                p = ax2.plot(log[metric_type + key], label=metric_type + key+str(i), linewidth=1)\n",
    "                c = p[-1].get_color()\n",
    "                ax2.plot(np.argmax(log[metric_type + key]), np.amax(log[metric_type + key]), \n",
    "                         '*', markersize=10, alpha=0.5, color=c)\n",
    "                ax2.set_title('max val acc: {0:.2f}'.format(np.amax(log['val_' + key]*100)))\n",
    "                ax2.set_ylabel('accuracy')\n",
    "                ax2.set_xlabel('epochs')\n",
    "    ax1.grid()\n",
    "    ax2.grid()\n",
    "    ax1.legend()\n",
    "    ax2.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, model_id in enumerate(range(17, 17+n_models)):\n",
    "    model_folder = os.path.join(output_baseline_dropout_folder, str(model_id))\n",
    "    test_pred_path = os.path.join(model_folder, 'test_pred.npy')\n",
    "    if not os.path.isfile(test_pred_path):\n",
    "        checkpoint_path = glob.glob(model_folder + '/weights.' + str(epochs).zfill(2) + '*.hdf5')[0]\n",
    "        model = cnn_mcdo(dropout=False)\n",
    "        print('loading weights for model {}...'.format(model_id))\n",
    "        model.load_weights(checkpoint_path)\n",
    "        print('inference...')\n",
    "        test_pred = model.predict(x_test)\n",
    "        np.save(test_pred_path, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_model_id = 17\n",
    "y_pred_sample = np.load(os.path.join(output_baseline_folder, str(sample_model_id), 'test_pred.npy'))\n",
    "shape = np.shape(y_pred_sample)\n",
    "all_test_preds = np.zeros((n_models, *shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = []\n",
    "for index, model_id in enumerate(range(17, 17 + n_models)):\n",
    "    y_pred = np.load(os.path.join(output_baseline_dropout_folder, str(model_id), 'test_pred.npy'))\n",
    "    all_test_preds[index] = y_pred\n",
    "    nll = log_loss(y_test, y_pred)\n",
    "    error = classification_error(y_test, y_pred)\n",
    "    br = brier_multi(y_test, y_pred)\n",
    "    calib_erros = CalibrationErrors(y_test, y_pred, bin_size=1 / 20., min_samples=0)\n",
    "    _, _, ece, mce, _ = calib_erros.calculate_calibration_errors()\n",
    "    d.append(OrderedDict({\"model\":model_id, \"nll\": nll, \"brier\": br, \"ece\": ece, \"mce\":mce, \"classification error\": error}))\n",
    "df_baselines_do = pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASELINE DROPOUT RESULTS (TEST SET)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "NLL\n",
      "1.054 ± 0.09\n",
      "--------------------\n",
      "BRIER\n",
      "0.481 ± 0.03\n",
      "--------------------\n",
      "ECE\n",
      "4.747 ± 1.67\n",
      "--------------------\n",
      "MCE\n",
      "12.815 ± 7.18\n",
      "--------------------\n",
      "CLASSIFICATION ERROR\n",
      "35.507 ± 3.09\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "print('Baseline Dropout Results (test set)'.upper())\n",
    "print('-'*100)\n",
    "metrics = ['nll', 'brier', 'ece', 'mce', 'classification error']\n",
    "for metric in metrics:\n",
    "    print('{0}'.format(metric.upper()))\n",
    "    print('{0:.3f} \\u00B1 {1:.2f}'.format(np.mean(df_baselines_do[metric]), np.std(df_baselines_do[metric])))\n",
    "    print('-'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mcdo_ensemble = 25\n",
    "for index, model_id in enumerate(range(17, 17+n_models)):\n",
    "    output_mcdo_folder = os.path.join(model_folder, 'mcdo')\n",
    "    test_pred_path = os.path.join(output_mcdo_folder, 'test_pred_mcdo.npy')\n",
    "    # if True:\n",
    "    if not os.path.isfile(test_pred_path):\n",
    "        model_folder = os.path.join(output_baseline_dropout_folder, str(model_id))\n",
    "        if not os.path.isdir(output_mcdo_folder):\n",
    "            os.mkdir(output_mcdo_folder)\n",
    "        checkpoint_path = glob.glob(model_folder + '/weights.' + str(epochs).zfill(2) + '*.hdf5')[0]\n",
    "        model = cnn_mcdo()\n",
    "        print('loading weights for model {}...'.format(model_id))\n",
    "        model.load_weights(checkpoint_path)\n",
    "        y_test_pred_ensemble = np.zeros((n_mcdo_ensemble, *y_test.shape))\n",
    "        for index2, seed in enumerate(range(17, 17 + n_mcdo_ensemble)):\n",
    "            np.random.seed(seed)\n",
    "            y_test_pred_dropout = model.predict(x_test, verbose=1)\n",
    "            y_test_pred_ensemble[index2] = y_test_pred_dropout\n",
    "        y_pred = np.mean(y_test_pred_ensemble, axis=0)\n",
    "        np.save(test_pred_path, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = list()\n",
    "for index, model_id in enumerate(range(17, 17 + n_models)):\n",
    "    model_mcdo_folder= os.path.join(output_baseline_dropout_folder, str(model_id),  'mcdo')\n",
    "    y_pred = np.load(os.path.join(model_mcdo_folder, 'test_pred_mcdo.npy'))\n",
    "    nll = log_loss(y_test, y_pred)\n",
    "    error = classification_error(y_test, y_pred)\n",
    "    br = brier_multi(y_test, y_pred)\n",
    "    calib_erros = CalibrationErrors(y_test, y_pred, bin_size=1 / 20., min_samples=0)\n",
    "    _, _, ece, mce, _ = calib_erros.calculate_calibration_errors()\n",
    "    d.append(OrderedDict({\"model\":model_id, \"nll\": nll, \"brier\": br, \"ece\": ece, \"mce\":mce, \"classification error\": error}))\n",
    "df_mcdo= pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>nll</th>\n",
       "      <th>brier</th>\n",
       "      <th>ece</th>\n",
       "      <th>mce</th>\n",
       "      <th>classification error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0.806739</td>\n",
       "      <td>0.384366</td>\n",
       "      <td>6.292623</td>\n",
       "      <td>11.408242</td>\n",
       "      <td>27.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0.806345</td>\n",
       "      <td>0.384910</td>\n",
       "      <td>7.010246</td>\n",
       "      <td>14.670719</td>\n",
       "      <td>27.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>0.784237</td>\n",
       "      <td>0.376619</td>\n",
       "      <td>6.639755</td>\n",
       "      <td>10.575379</td>\n",
       "      <td>26.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>0.788280</td>\n",
       "      <td>0.376721</td>\n",
       "      <td>7.558372</td>\n",
       "      <td>13.926969</td>\n",
       "      <td>26.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>0.779271</td>\n",
       "      <td>0.372362</td>\n",
       "      <td>6.814007</td>\n",
       "      <td>13.672411</td>\n",
       "      <td>26.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>0.836244</td>\n",
       "      <td>0.396332</td>\n",
       "      <td>5.918552</td>\n",
       "      <td>12.963861</td>\n",
       "      <td>28.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>0.798381</td>\n",
       "      <td>0.382612</td>\n",
       "      <td>7.648522</td>\n",
       "      <td>12.253529</td>\n",
       "      <td>27.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>24</td>\n",
       "      <td>0.793404</td>\n",
       "      <td>0.378043</td>\n",
       "      <td>6.417017</td>\n",
       "      <td>14.695643</td>\n",
       "      <td>26.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>25</td>\n",
       "      <td>0.821689</td>\n",
       "      <td>0.394077</td>\n",
       "      <td>6.335888</td>\n",
       "      <td>10.695846</td>\n",
       "      <td>28.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>26</td>\n",
       "      <td>0.784066</td>\n",
       "      <td>0.375467</td>\n",
       "      <td>7.400974</td>\n",
       "      <td>11.416102</td>\n",
       "      <td>26.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>27</td>\n",
       "      <td>0.792457</td>\n",
       "      <td>0.377792</td>\n",
       "      <td>7.774245</td>\n",
       "      <td>13.059641</td>\n",
       "      <td>26.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>28</td>\n",
       "      <td>0.807447</td>\n",
       "      <td>0.387402</td>\n",
       "      <td>7.563648</td>\n",
       "      <td>13.728781</td>\n",
       "      <td>27.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>29</td>\n",
       "      <td>0.829048</td>\n",
       "      <td>0.393199</td>\n",
       "      <td>6.315338</td>\n",
       "      <td>11.368985</td>\n",
       "      <td>28.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>30</td>\n",
       "      <td>0.787848</td>\n",
       "      <td>0.377833</td>\n",
       "      <td>7.570263</td>\n",
       "      <td>13.278161</td>\n",
       "      <td>27.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>31</td>\n",
       "      <td>0.782290</td>\n",
       "      <td>0.375358</td>\n",
       "      <td>7.864959</td>\n",
       "      <td>12.167218</td>\n",
       "      <td>26.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>32</td>\n",
       "      <td>0.803384</td>\n",
       "      <td>0.383994</td>\n",
       "      <td>8.158073</td>\n",
       "      <td>12.839205</td>\n",
       "      <td>27.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>33</td>\n",
       "      <td>0.793811</td>\n",
       "      <td>0.380075</td>\n",
       "      <td>7.092536</td>\n",
       "      <td>15.248897</td>\n",
       "      <td>26.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>34</td>\n",
       "      <td>0.769132</td>\n",
       "      <td>0.368802</td>\n",
       "      <td>7.462248</td>\n",
       "      <td>12.387911</td>\n",
       "      <td>26.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>35</td>\n",
       "      <td>0.787106</td>\n",
       "      <td>0.376100</td>\n",
       "      <td>7.375193</td>\n",
       "      <td>85.545222</td>\n",
       "      <td>26.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>36</td>\n",
       "      <td>0.794058</td>\n",
       "      <td>0.381018</td>\n",
       "      <td>6.085826</td>\n",
       "      <td>12.538225</td>\n",
       "      <td>27.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>37</td>\n",
       "      <td>0.799249</td>\n",
       "      <td>0.381620</td>\n",
       "      <td>7.979154</td>\n",
       "      <td>13.580827</td>\n",
       "      <td>27.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>38</td>\n",
       "      <td>0.797943</td>\n",
       "      <td>0.380261</td>\n",
       "      <td>6.978296</td>\n",
       "      <td>14.161261</td>\n",
       "      <td>27.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>39</td>\n",
       "      <td>0.797461</td>\n",
       "      <td>0.381589</td>\n",
       "      <td>7.410682</td>\n",
       "      <td>17.221305</td>\n",
       "      <td>27.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>40</td>\n",
       "      <td>0.797230</td>\n",
       "      <td>0.382829</td>\n",
       "      <td>6.717369</td>\n",
       "      <td>12.295562</td>\n",
       "      <td>27.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>41</td>\n",
       "      <td>0.809672</td>\n",
       "      <td>0.387111</td>\n",
       "      <td>7.330357</td>\n",
       "      <td>12.315549</td>\n",
       "      <td>27.77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    model       nll     brier       ece        mce  classification error\n",
       "0      17  0.806739  0.384366  6.292623  11.408242                 27.80\n",
       "1      18  0.806345  0.384910  7.010246  14.670719                 27.35\n",
       "2      19  0.784237  0.376619  6.639755  10.575379                 26.77\n",
       "3      20  0.788280  0.376721  7.558372  13.926969                 26.67\n",
       "4      21  0.779271  0.372362  6.814007  13.672411                 26.59\n",
       "5      22  0.836244  0.396332  5.918552  12.963861                 28.88\n",
       "6      23  0.798381  0.382612  7.648522  12.253529                 27.07\n",
       "7      24  0.793404  0.378043  6.417017  14.695643                 26.84\n",
       "8      25  0.821689  0.394077  6.335888  10.695846                 28.43\n",
       "9      26  0.784066  0.375467  7.400974  11.416102                 26.61\n",
       "10     27  0.792457  0.377792  7.774245  13.059641                 26.54\n",
       "11     28  0.807447  0.387402  7.563648  13.728781                 27.71\n",
       "12     29  0.829048  0.393199  6.315338  11.368985                 28.32\n",
       "13     30  0.787848  0.377833  7.570263  13.278161                 27.24\n",
       "14     31  0.782290  0.375358  7.864959  12.167218                 26.59\n",
       "15     32  0.803384  0.383994  8.158073  12.839205                 27.10\n",
       "16     33  0.793811  0.380075  7.092536  15.248897                 26.86\n",
       "17     34  0.769132  0.368802  7.462248  12.387911                 26.00\n",
       "18     35  0.787106  0.376100  7.375193  85.545222                 26.74\n",
       "19     36  0.794058  0.381018  6.085826  12.538225                 27.51\n",
       "20     37  0.799249  0.381620  7.979154  13.580827                 27.01\n",
       "21     38  0.797943  0.380261  6.978296  14.161261                 27.13\n",
       "22     39  0.797461  0.381589  7.410682  17.221305                 27.11\n",
       "23     40  0.797230  0.382829  6.717369  12.295562                 27.53\n",
       "24     41  0.809672  0.387111  7.330357  12.315549                 27.77"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mcdo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCDO RESULTS (TEST SET)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "NLL\n",
      "0.798 ± 0.01\n",
      "--------------------\n",
      "BRIER\n",
      "0.381 ± 0.01\n",
      "--------------------\n",
      "ECE\n",
      "7.109 ± 0.62\n",
      "--------------------\n",
      "MCE\n",
      "15.921 ± 14.29\n",
      "--------------------\n",
      "CLASSIFICATION ERROR\n",
      "27.207 ± 0.66\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "print('MCDO Results (test set)'.upper())\n",
    "print('-'*100)\n",
    "metrics = ['nll', 'brier', 'ece', 'mce', 'classification error']\n",
    "for metric in metrics:\n",
    "    print('{0}'.format(metric.upper()))\n",
    "    print('{0:.3f} \\u00B1 {1:.2f}'.format(np.mean(df_mcdo[metric]), np.std(df_mcdo[metric])))\n",
    "    print('-'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [df_baselines,\n",
    "df_ppe,\n",
    "df_ts,\n",
    "df_mcdo,\n",
    "df_deep_ensembles\n",
    "      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% nll\n",
      "1.063 $\\pm$ 0.03 &\n",
      "0.982 $\\pm$ 0.02 &\n",
      "0.956 $\\pm$ 0.02 &\n",
      "0.798 $\\pm$ 0.01 &\n",
      "0.709 $\\pm$ 0.00 \\\\\n",
      "----------------------------------------------------------------------------------------------------\n",
      "% brier\n",
      "0.469 $\\pm$ 0.01 &\n",
      "0.450 $\\pm$ 0.01 &\n",
      "0.447 $\\pm$ 0.01 &\n",
      "0.381 $\\pm$ 0.01 &\n",
      "0.336 $\\pm$ 0.00 \\\\\n",
      "----------------------------------------------------------------------------------------------------\n",
      "% ece\n",
      "11.718 $\\pm$ 0.72 &\n",
      "4.599 $\\pm$ 0.82 &\n",
      "1.318 $\\pm$ 0.26 &\n",
      "7.109 $\\pm$ 0.62 &\n",
      "8.862 $\\pm$ 0.25 \\\\\n",
      "----------------------------------------------------------------------------------------------------\n",
      "% mce\n",
      "22.828 $\\pm$ 11.87 &\n",
      "24.007 $\\pm$ 18.33 &\n",
      "10.748 $\\pm$ 7.48 &\n",
      "15.921 $\\pm$ 14.29 &\n",
      "18.757 $\\pm$ 10.50 \\\\\n",
      "----------------------------------------------------------------------------------------------------\n",
      "% classification error\n",
      "33.023 $\\pm$ 0.68 &\n",
      "32.949 $\\pm$ 0.74 &\n",
      "33.023 $\\pm$ 0.68 &\n",
      "27.207 $\\pm$ 0.66 &\n",
      "22.888 $\\pm$ 0.19 \\\\\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "metrics = ['nll', 'brier', 'ece', 'mce', 'classification error']\n",
    "for metric in metrics:\n",
    "    print('% {}'.format(metric))\n",
    "    for index, df in enumerate(dfs):\n",
    "        if index != len(dfs)-1:\n",
    "            print('{0:.3f} $\\pm$ {1:.2f} &'.format(np.mean(df[metric]), np.std(df[metric])))\n",
    "        else:\n",
    "            print('{0:.3f} $\\pm$ {1:.2f} \\\\\\\\'.format(np.mean(df[metric]), np.std(df[metric])))\n",
    "    print('-'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
